{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML simple paraphrase detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrsCpiskX70Vev1TeejQup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrublevskiyvitaliy/paraphrase_identification/blob/master/ML_simple_paraphrase_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hjFsc4mr-kb",
        "colab_type": "text"
      },
      "source": [
        "# Prepare data:\n",
        "We want to get parse trees of sentances.\n",
        "We will use stanford parser for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnNemAilqQC5",
        "colab_type": "text"
      },
      "source": [
        "### This is following the recomendations from:\n",
        "\n",
        "\n",
        "https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQWjZLrjptno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d203e92a-9399-49fe-c7ea-1d0e4dc59c90"
      },
      "source": [
        "!pip install stanza==1.0.0\n",
        "import stanza\n",
        "\n",
        "# Download the Stanford CoreNLP Java library and unzip it to a ./corenlp folder\n",
        "!echo \"Downloading CoreNLP...\"\n",
        "!wget \"http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\" -O corenlp.zip\n",
        "!unzip corenlp.zip\n",
        "!mv ./stanford-corenlp-full-2018-10-05 ./corenlp\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = \"./corenlp\"\n",
        "\n",
        "# Import client module\n",
        "from stanza.server import CoreNLPClient\n",
        "\n",
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(annotators=['parse'], memory='4G', endpoint='http://localhost:9001')\n",
        "print(client)\n",
        "\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanza==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza==1.0.0) (3.10.0)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from stanza==1.0.0) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza==1.0.0) (1.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza==1.0.0) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza==1.0.0) (2.21.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza==1.0.0) (46.1.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza==1.0.0) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->stanza==1.0.0) (0.16.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza==1.0.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza==1.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza==1.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza==1.0.0) (2020.4.5.1)\n",
            "Downloading CoreNLP...\n",
            "--2020-04-28 16:52:33--  http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2020-04-28 16:52:33--  https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 393239982 (375M) [application/zip]\n",
            "Saving to: ‘corenlp.zip’\n",
            "\n",
            "corenlp.zip         100%[===================>] 375.02M  11.4MB/s    in 28s     \n",
            "\n",
            "2020-04-28 16:53:02 (13.3 MB/s) - ‘corenlp.zip’ saved [393239982/393239982]\n",
            "\n",
            "Archive:  corenlp.zip\n",
            "   creating: stanford-corenlp-full-2018-10-05/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/README.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2018-10-05/sutime/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/build.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-javadoc.jar  \n",
            "   creating: stanford-corenlp-full-2018-10-05/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2018-10-05/patterns/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/Makefile  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LICENSE.txt  \n",
            "<stanza.server.client.CoreNLPClient object at 0x7f226a826cc0>\n",
            "Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-fa3bb9eb22d74b51.props -preload parse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVJYxQslyTNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stanza.protobuf.CoreNLP_pb2 import ParseTree\n",
        "\n",
        "def parse_sentances(sentances):\n",
        "  data = []\n",
        "  for s in sentances:\n",
        "    document = client.annotate(s)\n",
        "    tree = document.sentence[0].parseTree\n",
        "    data.append(tree.SerializeToString())\n",
        "  return data\n",
        "\n",
        "def get_parse_tree(sentance):\n",
        "  document = client.annotate(s)\n",
        "  return document.sentence[0].parseTree\n",
        "  # todo: use precomputed data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY1ODwqB3XXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Supress output of the cell\n",
        "%%capture\n",
        "def download_pickle():\n",
        "    \"\"\"\n",
        "      Downloading pickle\n",
        "    \"\"\" \n",
        "    files = [\n",
        "      'griff4692/paraphrase-detection/contents/vocab/word2Idx.pk',\n",
        "      'griff4692/paraphrase-detection/contents/vocab/idx2Word.pk',\n",
        "      'griff4692/paraphrase-detection/contents/vocab/idx2Emb_100.pk',\n",
        "    ]\n",
        "    for f in files:\n",
        "       !curl --remote-name \\\n",
        "          -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "          --location https://api.github.com/repos/{f}\n",
        "\n",
        "download_pickle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RT2wOz15GUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ccb7090c-50ba-430c-f1d3-df91882ba513"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 17:45:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-28 17:45:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-28 17:45:38--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  1.99MB/s    in 6m 29s  \n",
            "\n",
            "2020-04-28 17:52:07 (2.12 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed33lXNH1qz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"StanfordParser\"] = \"/content/drive/My Drive/phd/stanford-parser-3.9.2-models.jar\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZVTgreCiReb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vocab.py\n",
        "import numpy as np\n",
        "from nltk.parse import CoreNLPParser\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "class Vocab:\n",
        "    WORD2IDX = 'word2Idx.pk'\n",
        "    IDX2WORD = 'idx2Word.pk'\n",
        "    IDX2EMB = 'idx2Emb_100.pk'\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.word2Idx = {}\n",
        "        self.idx2Word = []\n",
        "        self.idx2Emb = []\n",
        "        self.parser = CoreNLPParser(url='http://localhost:9001')\n",
        "\n",
        "    def load(self, embed_size):\n",
        "        self.word2Idx = pickle.load(open(self.WORD2IDX, 'rb'))\n",
        "        self.idx2Word = pickle.load(open(self.IDX2WORD, 'rb'))\n",
        "        self.idx2Emb = pickle.load(open(\"idx2Emb_100.pk\", 'rb'))\n",
        "\n",
        "    def save(self):\n",
        "        pickle.dump(self.word2Idx, open(self.WORD2IDX, 'wb'))\n",
        "        pickle.dump(self.idx2Word, open(self.IDX2WORD, 'wb'))\n",
        "        pickle.dump(self.idx2Emb, open(self.IDX2EMB, 'wb'))\n",
        "\n",
        "\n",
        "    def is_fraction(self, string):\n",
        "        split = string.split('/')\n",
        "\n",
        "        return len(split) == 2 and split[0].isdigit() and split[1].isdigit()\n",
        "\n",
        "    def tokenizer(self, word_str):\n",
        "        word_str = word_str.decode('utf-8')\n",
        "        parsed_sentence = list(self.parser.raw_parse(word_str))[0].leaves()\n",
        "\n",
        "        tokens = []\n",
        "        for (i, token) in enumerate(parsed_sentence):\n",
        "            token = token.lower()\n",
        "            if token.isdigit() and i < len(parsed_sentence) - 1:\n",
        "                if self.is_fraction(parsed_sentence[i + 1]):\n",
        "                    fraction = token + '-' + parsed_sentence[i + 1]\n",
        "                    tokens.append(token + '-' + parsed_sentence[i + 1])\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def get(self, token):\n",
        "        return self.word2Idx[token] if token in self.word2Idx else self.unk_idx()\n",
        "\n",
        "    def get_word(self, idx):\n",
        "        if idx < 0 or idx >= self.size():\n",
        "            return UNK\n",
        "        else:\n",
        "            return self.idx2Word[idx]\n",
        "\n",
        "    def tokens_to_idxs(self, tokens):\n",
        "        return [self.get(token) for token in tokens]\n",
        "\n",
        "    def sentence_to_idxs(self, sentence_str):\n",
        "        tokens = self.tokenizer(sentence_str)\n",
        "        return self.tokens_to_idxs(tokens)\n",
        "\n",
        "    def build(self, sentences, path_to_embed, emb_dim):\n",
        "        self.emb_dim = emb_dim\n",
        "        self.max_sent_length = 0\n",
        "        for sentence in sentences:\n",
        "            tokens_1 = self.tokenizer(sentence[0])\n",
        "            tokens_2 = self.tokenizer(sentence[1])\n",
        "\n",
        "            len1 = len(tokens_1)\n",
        "            len2 = len(tokens_2)\n",
        "\n",
        "            self.max_sent_length = np.max([len1, len2, self.max_sent_length])\n",
        "\n",
        "            for token in tokens_1 + tokens_2:\n",
        "                if token not in self.word2Idx:\n",
        "                    idx = len(self.idx2Word)\n",
        "                    self.idx2Word.append(token)\n",
        "                    self.word2Idx[token] = idx\n",
        "\n",
        "        self.build_emb_matrix(path_to_embed, emb_dim)\n",
        "\n",
        "    def unk_idx(self):\n",
        "        return self.size()\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.idx2Word)\n",
        "\n",
        "    def build_emb_matrix(self, path_to_embed, emb_dim):\n",
        "        self.emb_dim = emb_dim\n",
        "        self.idx2Emb = np.zeros((self.size() + 1, emb_dim), dtype='float')\n",
        "\n",
        "        hits = 0\n",
        "\n",
        "        embeddings = open(path_to_embed)\n",
        "        for embedding in embeddings:\n",
        "            split = embedding.split()\n",
        "            word = split[0]\n",
        "\n",
        "            if word in self.word2Idx:\n",
        "                vals =  np.array([float(num) for num in split[1:]])\n",
        "                self.idx2Emb[self.word2Idx[word]] = vals\n",
        "                hits += 1\n",
        "\n",
        "        print(\"%s out of %s word embeddings initialized.\" % (hits, self.size()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5U49g9Hl1Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from nltk.parse.stanford import StanfordParser\n",
        "from nltk.parse import CoreNLPParser\n",
        "import os\n",
        "from queue import Queue\n",
        "\n",
        "parser = CoreNLPParser(url='http://localhost:9001')\n",
        "\n",
        "\n",
        "def glob_compound_leaves(leaves, compound1, compound2 = None):\n",
        "    if compound2 is None:\n",
        "        return leaves\n",
        "\n",
        "    for (i, leaf) in enumerate(leaves):\n",
        "        if compound1 == leaf and compound2 == leaves[i + 1]:\n",
        "            # both are equal to compound\n",
        "            leaves[i] = compound1 + \"-\" + compound2\n",
        "            leaves = leaves[:i + 1] + leaves[i + 2:]\n",
        "            return leaves\n",
        "\n",
        "    raise Exception(\"Should be able to find compount leaf words!\")\n",
        "\n",
        "\n",
        "def build_tree(raw_sent, vocab):\n",
        "    tree = list(parser.raw_parse(raw_sent))[0]\n",
        "\n",
        "    leaves = [leaf.lower() for leaf in tree.leaves()]\n",
        "\n",
        "    id_counter = 0\n",
        "\n",
        "    rootNode = TreeNode(tree.label(), tree.height() - 1, id_counter, -1, None, None)\n",
        "    sentence = TreeSentence(rootNode)\n",
        "\n",
        "    q = Queue()\n",
        "    q.put((tree, 0, id_counter)) # nltk Tree Object, level, current_id\n",
        "\n",
        "    while not q.empty():\n",
        "        curr_subtree, curr_level, curr_id = q.get()\n",
        "\n",
        "        for subtree in curr_subtree:\n",
        "            id_counter += 1\n",
        "\n",
        "            subtree_node = TreeNode(subtree.label(), subtree.height() - 1, id_counter, curr_id, None, None)\n",
        "            node_idx = sentence.add_to_level(curr_level + 1, subtree_node)\n",
        "\n",
        "            if type(subtree[0]) == unicode:\n",
        "                word = subtree[0].lower()\n",
        "                if len(subtree) > 1:\n",
        "                    word2 = subtree[1].lower()\n",
        "                    leaves = glob_compound_leaves(leaves, word, word2)\n",
        "                    word += \"-\" + word2\n",
        "\n",
        "                subtree_node.word = word\n",
        "                subtree_node.token = vocab.get(word)\n",
        "            else:\n",
        "                q.put((subtree, curr_level + 1, id_counter))\n",
        "\n",
        "    sentence.add_leaves(leaves, vocab)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "class TreeSentence:\n",
        "    def __init__ (self, levels, leaves = []):\n",
        "        self.leaves = leaves\n",
        "        if isinstance(levels, TreeNode):\n",
        "            root = levels\n",
        "            self.levels = [[] for i in range(root.get_height())]\n",
        "            self.levels[0].append(root)\n",
        "        else:\n",
        "            self.levels = levels\n",
        "            for (r, level) in enumerate(levels):\n",
        "                for(c, node_dict) in enumerate(level):\n",
        "                    if(not isinstance(node_dict, TreeNode)):\n",
        "                        self.levels[r][c] = TreeNode(**node_dict)\n",
        "\n",
        "    def add_to_level(self, idx, node):\n",
        "        self.levels[idx].append(node)\n",
        "        return len(self.levels[idx]) - 1\n",
        "\n",
        "    def add_leaves(self, leaves, vocab):\n",
        "        self.leaves = [vocab.get(leaf) for leaf in leaves]\n",
        "\n",
        "    def get_height(self):\n",
        "        return self.levels[0][0].get_height()\n",
        "\n",
        "    def render(self, vocab=None):\n",
        "        for (i, level) in enumerate(self.levels):\n",
        "            print(\"Level #\" + str(i) + \"\\n\")\n",
        "            for (j, node) in enumerate(level):\n",
        "                node.pretty_print(self.get_children(i, j), vocab)\n",
        "            print (\"\\n\")\n",
        "\n",
        "    def get_children(self, level, idx):\n",
        "        parent_node = self.levels[level][idx]\n",
        "\n",
        "        if level >= len(self.levels) - 1:\n",
        "            return []\n",
        "        else:\n",
        "            return [\n",
        "                (i, node) for (i, node) in enumerate(self.levels[level + 1])\n",
        "                    if node.parent_id == parent_node.my_id\n",
        "            ]\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, label, height, my_id, parent_id, token, word):\n",
        "        self.label = label\n",
        "        self.token = token\n",
        "        self.word = word\n",
        "        self.my_id = my_id\n",
        "        self.height = height\n",
        "        self.parent_id = parent_id\n",
        "\n",
        "    def get_height(self):\n",
        "        return self.height\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.get_height() == 1\n",
        "\n",
        "    def pretty_print(self, child_ids, vocab=None):\n",
        "        child_ids = [str(child_id[0]) for child_id in child_ids]\n",
        "        print ('ID = ( ' + str(self.my_id) + ' )')\n",
        "        print ('\\t--> Label = ' + self.label)\n",
        "        print ('\\t--> Height = ' + str(self.height))\n",
        "        print ('\\t--> Parent ID = ( ' + str(self.parent_id) + ' )')\n",
        "        print ('\\t--> Child IDs = [ ' + \", \".join(child_ids) + ' ]')\n",
        "        if self.word is not None:\n",
        "            print ('\\t--> Word = ' + self.word + ' (' + str(self.token) + ')')\n",
        "\n",
        "# if __name__== '__main__':\n",
        "#     sentence = \"the cats from new york catch mice\".decode('utf-8')\n",
        "#     vocab = Vocab()\n",
        "#     vocab.load(100)\n",
        "#     tree_sentence = build_tree(sentence, vocab)\n",
        "#     tree_sentence.render(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LeytvvY1lzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "a4444aa7-ce1c-4f68-912a-8f0d399e80b5"
      },
      "source": [
        "sentence = \"the cats from new york catch mice\"\n",
        "vocab = Vocab()\n",
        "vocab.load(100)\n",
        "tree_sentence = build_tree(sentence, vocab)\n",
        "tree_sentence.render(vocab)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-461ec0ed2bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the cats from new york catch mice\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtree_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtree_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-c08b4b016bd9>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, embed_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2Idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORD2IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2Word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIDX2WORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2Emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIDX2EMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'."
          ]
        }
      ]
    }
  ]
}