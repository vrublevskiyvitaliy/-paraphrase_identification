{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All installs here\n",
    "\n",
    "# Download medium model\n",
    "\n",
    "# Uncomment to install first time\n",
    "#! pip install spacy\n",
    "#! python -m spacy download en_core_web_lg\n",
    "# !pip install --upgrade pip\n",
    "# !pip install numpy==1.18\n",
    "# !pip install scipy==1.1.0\n",
    "# !pip install scikit-learn==0.21.3\n",
    "# !pip install networkx==2.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All imports and installs should be here\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import wordpunct_tokenize\n",
    "import operator\n",
    "import re, string\n",
    "import math\n",
    "import spacy\n",
    "import copy\n",
    "\n",
    "from nltk import Tree\n",
    "# Space module import\n",
    "import en_core_web_md\n",
    "import en_core_web_lg\n",
    "# NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
    "import networkx as nx\n",
    "from networkx import __version__ as nxv\n",
    "import networkx.algorithms as networkx_algorithms\n",
    "\n",
    "# linear_sum_assignment Hungarian algorithm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegressionCV, PassiveAggressiveClassifier, RidgeClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# Nice progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "# Used in plotting graphs \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from fractions import Fraction\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All constants\n",
    "\n",
    "COLAB_ENV = \"colab\"\n",
    "LOCAL_ENV = \"local\"\n",
    "\n",
    "SENTENCE_START_TOKEN = \"sentence_start\"\n",
    "SENTENCE_END_TOKEN = \"sentence_end\"\n",
    "UNKNOWN_TOKEN = \"unknown_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_spacy_module():\n",
    "  return en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To be able run this notebook from Google Colab and localy\n",
    "\n",
    "def get_running_env():\n",
    "  current_path = os.getcwd()\n",
    "  if current_path == \"/content\":\n",
    "    return COLAB_ENV\n",
    "  return LOCAL_ENV\n",
    "\n",
    "RUNNING_ENV = get_running_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Supress output of the cell\n",
    "\n",
    "def download_corpus():\n",
    "    \"\"\"\n",
    "      Downloading corpus files for colab research.\n",
    "    \"\"\" \n",
    "    if RUNNING_ENV == LOCAL_ENV:\n",
    "      return\n",
    "    files = [\n",
    "      'vrublevskiyvitaliy/paraphrase_identification/contents/dataset/msr-paraphrase-corpus/msr_paraphrase_train.txt',\n",
    "      'vrublevskiyvitaliy/paraphrase_identification/contents/dataset/msr-paraphrase-corpus/msr_paraphrase_test.txt',\n",
    "    ]\n",
    "    for f in files:\n",
    "       !curl --remote-name \\\n",
    "          -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "          --location https://api.github.com/repos/{f}\n",
    "\n",
    "# download_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_location():\n",
    "  return \"\" if RUNNING_ENV == COLAB_ENV else \"./../dataset/msr-paraphrase-corpus/\"\n",
    "\n",
    "def add_start_end_sentence_tokens(s):\n",
    "  return \"%s %s %s\" % (SENTENCE_START_TOKEN, s, SENTENCE_END_TOKEN)\n",
    "\n",
    "def load_data(_preprocess_sentence=None, _train=False, _test=False):\n",
    "    \"Load the MSRP dataset.\"\n",
    "    loc = get_data_location()\n",
    "    trainloc = loc + 'msr_paraphrase_train.txt'\n",
    "    testloc = loc + 'msr_paraphrase_test.txt'\n",
    "\n",
    "    if _preprocess_sentence is None:\n",
    "      _preprocess_sentence = lambda x: x\n",
    "\n",
    "    sent1_train, sent2_train, sent1_test, sent2_test = [], [], [], []\n",
    "    label_train, label_dev, label_test = [], [], []\n",
    "\n",
    "    if _train:\n",
    "        with open(trainloc, 'r', encoding='utf8') as f:\n",
    "            f.readline()  # skipping the header of the file\n",
    "            for line in f:\n",
    "                text = line.strip().split('\\t')\n",
    "                sent1_train.append(_preprocess_sentence(text[3]))\n",
    "                sent2_train.append(_preprocess_sentence(text[4]))\n",
    "                label_train.append(int(text[0]))\n",
    "\n",
    "    if _test:\n",
    "        with open(testloc, 'r', encoding='utf8') as f:\n",
    "            f.readline()  # skipping the header of the file\n",
    "            for line in f:\n",
    "                text = line.strip().split('\\t')\n",
    "                sent1_test.append(_preprocess_sentence(text[3]))\n",
    "                sent2_test.append(_preprocess_sentence(text[4]))\n",
    "                label_test.append(int(text[0]))\n",
    "\n",
    "    if _train and _test:\n",
    "        return [sent1_train, sent2_train], [sent1_test, sent2_test], [label_train, label_test]\n",
    "    elif _train:\n",
    "        return [sent1_train, sent2_train], label_train\n",
    "    elif _test:\n",
    "        return [sent1_test, sent2_test], label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_data(_preprocess_sentence=None, _train=True, _test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sample_data(index=0):\n",
    "  all_data = load_data(_preprocess_sentence=None, _train=True, _test=False)\n",
    "  return all_data[0][0][index], all_data[0][1][index], all_data[1][index]\n",
    "\n",
    "def get_sample(index=0):\n",
    "  all_data = load_data(_preprocess_sentence=None, _train=True, _test=False)\n",
    "  return {'s1': all_data[0][0][index], 's2': all_data[0][1][index], 'label': all_data[1][index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = get_sample_data()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = get_spacy_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dependancy_graph(s, display=False):\n",
    "  doc = nlp(s)\n",
    "  if display:\n",
    "    spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "  edges = []\n",
    "  nodes = [{\n",
    "      \"node\": \"ROOT\",\n",
    "      \"token\": None,\n",
    "      \"is_fake\": True, \n",
    "  }]\n",
    "  for token in doc:\n",
    "    nodes.append({\n",
    "        \"node\": token.text,\n",
    "        \"token\": token,\n",
    "        \"is_fake\": False,\n",
    "    })\n",
    "    if token.dep_ == \"ROOT\":\n",
    "      edges.append({\n",
    "        \"start\": \"ROOT\",\n",
    "        \"end\": token.text,\n",
    "        \"start_node_id\": 0,\n",
    "        \"end_node_id\": token.i + 1,\n",
    "        \"type\": token.dep_\n",
    "      })\n",
    "    else:\n",
    "      edges.append({\n",
    "        \"start\": token.head.text,\n",
    "        \"end\": token.text,\n",
    "        \"start_node_id\":  token.head.i + 1,\n",
    "        \"end_node_id\": token.i + 1,\n",
    "        \"type\": token.dep_\n",
    "      })\n",
    "  return {\"nodes\": nodes, \"edges\": edges}\n",
    "\n",
    "graph1 = get_dependancy_graph(sample[0], False)\n",
    "graph2 = get_dependancy_graph(sample[1], False)\n",
    "print(graph1)\n",
    "print(graph2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here I try to explore diferent Graph similarity algorithms:\n",
    "\n",
    "First approach\n",
    "1.   Map nodes using Hungarian algorithm\n",
    "2.   Count Graph Edit Distance GED also using Hungarian algorithm\n",
    "\n",
    "Ideas to try:\n",
    "* add treshold for node matching, only if > 0.5 for example merge nodes\n",
    "\n",
    "Next ideas to try: \n",
    "* From graphs get graph based entities like path, or subgraphs.\n",
    "* Combine vectors in these entities\n",
    "* Count # of entities with some similarity threshold\n",
    "\n",
    "Look at different structural graph features. Try to look at https://networkx.github.io/documentation/latest/reference/algorithms/index.html\n",
    "\n",
    "\n",
    "One example could be Wiener index (need to think on normalization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code is taken from https://github.com/Jacobe2169/ged4py\n",
    "\n",
    "class EdgeGraph():\n",
    "    def __init__(self, init_node, nodes):\n",
    "        self.init_node=init_node\n",
    "        self.nodes_ = nodes\n",
    "\n",
    "    def nodes(self):\n",
    "        return self.nodes_\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.nodes)\n",
    "    def __len__(self):\n",
    "        return len(self.nodes_)\n",
    "\n",
    "class AbstractGraphEditDistance(object):\n",
    "    def __init__(self, g1, g2):\n",
    "        self.g1 = g1\n",
    "        self.g2 = g2\n",
    "\n",
    "    def normalized_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the graph edit distance between graph g1 & g2\n",
    "        The distance is normalized on the size of the two graphs.\n",
    "        This is done to avoid favorisation towards smaller graphs\n",
    "        \"\"\"\n",
    "        avg_graphlen = len(self.g1) + len(self.g2)\n",
    "        return self.distance() / avg_graphlen\n",
    "\n",
    "    def distance(self):\n",
    "        return sum(self.edit_costs())\n",
    "\n",
    "    def edit_costs(self):\n",
    "        cost_matrix = self.create_cost_matrix()\n",
    "        row_ind,col_ind = linear_sum_assignment(cost_matrix)\n",
    "        return [cost_matrix[row_ind[i]][col_ind[i]] for i in range(len(row_ind))]\n",
    "\n",
    "    def create_cost_matrix(self):\n",
    "        \"\"\"\n",
    "        Creates a |N+M| X |N+M| cost matrix between all nodes in\n",
    "        graphs g1 and g2\n",
    "        Each cost represents the cost of substituting,\n",
    "        deleting or inserting a node\n",
    "        The cost matrix consists of four regions:\n",
    "        substitute \t| insert costs\n",
    "        -------------------------------\n",
    "        delete \t\t| delete -> delete\n",
    "        The delete -> delete region is filled with zeros\n",
    "        \"\"\"\n",
    "        n = len(self.g1)\n",
    "        m = len(self.g2)\n",
    "        cost_matrix = np.zeros((n+m,n+m))\n",
    "        #cost_matrix = [[0 for i in range(n + m)] for j in range(n + m)]\n",
    "        nodes1 = self.g1.nodes() if float(nxv) < 2 else list(self.g1.nodes())\n",
    "        nodes2 = self.g2.nodes() if float(nxv) < 2 else list(self.g2.nodes())\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                cost_matrix[i,j] = self.substitute_cost(nodes1[i], nodes2[j])\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                cost_matrix[i+n,j] = self.insert_cost(i, j, nodes2)\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                cost_matrix[j,i+m] = self.delete_cost(i, j, nodes1)\n",
    "\n",
    "        self.cost_matrix = cost_matrix\n",
    "        return cost_matrix\n",
    "\n",
    "    def insert_cost(self, i, j):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def delete_cost(self, i, j):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def substitute_cost(self, nodes1, nodes2):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def print_matrix(self):\n",
    "        print(\"cost matrix:\")\n",
    "        for column in self.create_cost_matrix():\n",
    "            for row in column:\n",
    "                if row == sys.maxsize:\n",
    "                    print (\"inf\\t\")\n",
    "                else:\n",
    "                    print (\"%.2f\\t\" % float(row))\n",
    "            print(\"\")\n",
    "\n",
    "class EdgeEditDistance(AbstractGraphEditDistance):\n",
    "    \"\"\"\n",
    "    Calculates the graph edit distance between two edges.\n",
    "    A node in this context is interpreted as a graph,\n",
    "    and edges are interpreted as nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g1, g2):\n",
    "        AbstractGraphEditDistance.__init__(self, g1, g2)\n",
    "\n",
    "    def insert_cost(self, i, j, nodes2):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        return sys.maxsize\n",
    "\n",
    "    def delete_cost(self, i, j, nodes1):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        return sys.maxsize\n",
    "\n",
    "    def substitute_cost(self, edge1, edge2):\n",
    "        if edge1 == edge2:\n",
    "            return 0.\n",
    "        return 1\n",
    "\n",
    "class GraphEditDistance(AbstractGraphEditDistance):\n",
    "    def __init__(self, g1, g2):\n",
    "        AbstractGraphEditDistance.__init__(self, g1, g2)\n",
    "\n",
    "    def substitute_cost(self, node1, node2):\n",
    "        return self.relabel_cost(node1, node2) + self.edge_diff(node1, node2)\n",
    "\n",
    "    def relabel_cost(self, node1, node2):\n",
    "        if node1 == node2:\n",
    "            return 0.\n",
    "        else:\n",
    "            return 1.\n",
    "\n",
    "    def delete_cost(self, i, j, nodes1):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        return sys.maxsize\n",
    "\n",
    "    def insert_cost(self, i, j, nodes2):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        else:\n",
    "            return sys.maxsize\n",
    "\n",
    "    def pos_insdel_weight(self, node):\n",
    "        return 1\n",
    "\n",
    "    def edge_diff(self, node1, node2):\n",
    "        edges1 = list(self.g1.edge[node1].keys()) if float(nxv) < 2 else list(self.g1.edges(node1))\n",
    "        edges2 = list(self.g2.edge[node2].keys()) if float(nxv) < 2 else list(self.g2.edges(node2))\n",
    "        if len(edges1) == 0 or len(edges2) == 0:\n",
    "            return max(len(edges1), len(edges2))\n",
    "\n",
    "        edit_edit_dist = EdgeEditDistance(EdgeGraph(node1,edges1), EdgeGraph(node2,edges2))\n",
    "        return edit_edit_dist.normalized_distance()\n",
    "\n",
    "def compare_graphs(g1, g2, print_details=False, use_normalized=True):\n",
    "    ged = GraphEditDistance(g1, g2)\n",
    "\n",
    "    if print_details:\n",
    "        ged.print_matrix()\n",
    "\n",
    "    return ged.normalized_distance() if use_normalized else ged.distance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HungarianGraphNodesMatcher:\n",
    "\n",
    "  def __init__(self, _g1, _g2, threshold=0.5):\n",
    "    self.g1 = _g1\n",
    "    self.g2 = _g2\n",
    "    self.node_threshold = threshold\n",
    "    self.create_cost_matrix()\n",
    "    self.solve_linear_sum_assignment()\n",
    "    self.match_nodes()\n",
    "\n",
    "  def set_threshold(self, threshold):\n",
    "    self.node_threshold = threshold\n",
    "    self.match_nodes()\n",
    "\n",
    "  def create_cost_matrix(self):\n",
    "    self.matrix = np.zeros((len(self.g1[\"nodes\"]), len(self.g2[\"nodes\"])))\n",
    "    for i1, n1 in enumerate(self.g1[\"nodes\"]):\n",
    "       for i2, n2 in enumerate(self.g2[\"nodes\"]):\n",
    "         if (not n1[\"is_fake\"] and not n2[\"is_fake\"] and \n",
    "            n1[\"token\"].has_vector and n2[\"token\"].has_vector):\n",
    "           self.matrix[i1][i2] = n1[\"token\"].similarity(n2[\"token\"])\n",
    "         elif n1[\"is_fake\"] == n2[\"is_fake\"]:\n",
    "           self.matrix[i1][i2] = n1[\"node\"] == n2[\"node\"]\n",
    "         else:\n",
    "           self.matrix[i1][i2] = 0\n",
    "\n",
    "    # Now we need to fleep scores, because Hungarian is trying to minimize\n",
    "    self.cost = np.subtract(np.full(self.matrix.shape, 1), self.matrix)\n",
    "\n",
    "  def get_pandas_matrix(self):\n",
    "    df = pd.DataFrame(\n",
    "        data=self.matrix,\n",
    "        index=np.array([n[\"node\"] for n in self.g1[\"nodes\"]]),\n",
    "        columns=np.array([n[\"node\"] for n in self.g2[\"nodes\"]])\n",
    "      )\n",
    "    \n",
    "    return df\n",
    "\n",
    "  def solve_linear_sum_assignment(self):\n",
    "    row_ind, col_ind = linear_sum_assignment(self.cost)\n",
    "    \n",
    "    self.row_ind = row_ind\n",
    "    self.col_ind = col_ind\n",
    "\n",
    "  def match_nodes(self):\n",
    "    self.graph1_to_graph2 = {\n",
    "        item[0]: item[1] \n",
    "        for item in zip(self.row_ind, self.col_ind)\n",
    "        if self.matrix[item[0]][item[1]] > self.node_threshold\n",
    "    }\n",
    "\n",
    "  def create_node_aliases(self):\n",
    "    for id1, n1 in enumerate(self.g1[\"nodes\"]):\n",
    "      n1[\"alias\"] = \"G1_\" + str(id1) + n1[\"node\"]\n",
    "    for id2, n2 in enumerate(self.g2[\"nodes\"]):\n",
    "      n2[\"alias\"] = \"G2_\" + str(id2) + n2[\"node\"] \n",
    "    for id1, id2 in self.graph1_to_graph2.items():\n",
    "      n1 = self.g1[\"nodes\"][id1]\n",
    "      n2 = self.g2[\"nodes\"][id2]\n",
    "      n1[\"alias\"] = \"G1_\" + str(id1) + \"_\" + n1[\"node\"] + \"_G2_\" + str(id2) + \"_\" + n2[\"node\"]\n",
    "      n2[\"alias\"] = n1[\"alias\"]\n",
    "\n",
    "  def build_graph(self, g):\n",
    "    nx_g = nx.Graph()\n",
    "    for edge in g[\"edges\"]:\n",
    "      start_node = g[\"nodes\"][edge[\"start_node_id\"]]\n",
    "      end_node = g[\"nodes\"][edge[\"end_node_id\"]]\n",
    "      nx_g.add_edge(start_node[\"alias\"], end_node[\"alias\"])\n",
    "    return nx_g\n",
    "\n",
    "  def get_converted_graphs(self):\n",
    "    self.create_node_aliases()\n",
    "    g1 = self.build_graph(self.g1)\n",
    "    g2 = self.build_graph(self.g2)\n",
    "    return g1, g2\n",
    "\n",
    "  def print_matched_nodes(self):\n",
    "    print (\"Graph 1  =>   Graph 2\")\n",
    "    for id1, id2 in self.graph1_to_graph2.items():\n",
    "      print(f\"{self.g1['nodes'][id1]['node']}    =>   {self.g2['nodes'][id2]['node']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphBuilder:\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  @classmethod\n",
    "  def build_nx_graph_from_dt(cls, g):\n",
    "    nx_g = nx.Graph()\n",
    "    for index, node in enumerate(g[\"nodes\"]):\n",
    "      nx_g.add_node(index, node=node['node'], token=node['token'], is_fake=node['is_fake'])\n",
    "    for edge in g[\"edges\"]:\n",
    "      nx_g.add_edge(edge[\"start_node_id\"], edge[\"end_node_id\"], dependancy_type=edge[\"type\"])\n",
    "    return nx_g\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def build_nx_graph_from_sentance(cls, s):\n",
    "    graph = get_dependancy_graph(s, False)\n",
    "    return cls.build_nx_graph_from_dt(graph)\n",
    "\n",
    "  @classmethod\n",
    "  def get_root_node(cls, g):\n",
    "    main_root_node = [n for n, _ in g.adj[0].items()][0]\n",
    "    return g.nodes[main_root_node]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_matcher = HungarianGraphNodesMatcher(graph1, graph2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_matcher.print_matched_nodes()\n",
    "len(node_matcher.graph1_to_graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = node_matcher.get_pandas_matrix()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_matcher.print_matched_nodes()\n",
    "g1, g2 = node_matcher.get_converted_graphs()\n",
    "score = compare_graphs(g1, g2)\n",
    "print(f\"Similarity score {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(g1, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_scores(limit=10):\n",
    "  result = []\n",
    "  for i in range(limit):\n",
    "    s1 = data[0][0][i]\n",
    "    s2 = data[0][1][i]\n",
    "    label = data[1][i]\n",
    "    graph1 = get_dependancy_graph(s1, False)\n",
    "    graph2 = get_dependancy_graph(s2, False)\n",
    "    node_matcher = HungarianGraphNodesMatcher(graph1, graph2, 0.8)  \n",
    "    g1, g2 = node_matcher.get_converted_graphs()\n",
    "    score = compare_graphs(g1, g2)\n",
    "    result.append((s1, s2, label, score))\n",
    "  return result\n",
    "\n",
    "result = calculate_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in result:\n",
    "  print (f\"Label {d[2]}  |   Score {d[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepared_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "  @classmethod\n",
    "  def get_train_data(cls):\n",
    "    [sent1_train, sent2_train], label_train = load_data(_preprocess_sentence=None, _train=True, _test=False)\n",
    "    return [\n",
    "      {\"s1\": item[0], \"s2\": item[1], \"label\": item[2]}\n",
    "      for item in zip(sent1_train, sent2_train, label_train)       \n",
    "    ]\n",
    "\n",
    "  @classmethod\n",
    "  def get_test_data(cls):\n",
    "    [sent1_test, sent2_test], label_test = load_data(_preprocess_sentence=None, _train=False, _test=True)\n",
    "    \n",
    "    return [\n",
    "      {\"s1\": item[0], \"s2\": item[1], \"label\": item[2]}\n",
    "      for item in zip(sent1_test, sent2_test, label_test)       \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(feature_generator, force=False, limit=100000):\n",
    "  feature_name = feature_generator.NAME\n",
    "  global prepared_data\n",
    "  if feature_name not in prepared_data or force:\n",
    "    train_data = DataGenerator.get_train_data()\n",
    "    test_data = DataGenerator.get_test_data()\n",
    "    train_X = [\n",
    "      feature_generator.get_features(item['s1'], item['s2'])\n",
    "      for item in tqdm(train_data)\n",
    "    ]\n",
    "    train_Y = [item['label'] for item in train_data]\n",
    "\n",
    "    test_X = [\n",
    "      feature_generator.get_features(item['s1'], item['s2'])\n",
    "      for item in tqdm(test_data)\n",
    "    ]\n",
    "    test_Y = [item['label'] for item in test_data]\n",
    "    \n",
    "    features = {}\n",
    "    features['train_X'] = train_X\n",
    "    features['train_Y'] = train_Y\n",
    "    features['test_X'] = test_X\n",
    "    features['test_Y'] = test_Y\n",
    "    prepared_data[feature_name] = features\n",
    "\n",
    "def verbose_data(feature_generator, limit=10, offset=0):\n",
    "    train_data = DataGenerator.get_train_data()\n",
    "    for item in train_data[offset:limit]:\n",
    "      features = feature_generator.get_features(item['s1'], item['s2'])\n",
    "      print(item['s1'])\n",
    "      print(item['s2'])\n",
    "      print(f\"Label {item['label']}\")\n",
    "      print(features)\n",
    "      print(\"*\"* 8)\n",
    "\n",
    "def get_metrics(feature_generator, classificator, force=False, limit=None, features_bitmap=None):\n",
    "  prepare_data(feature_generator, force, limit)\n",
    "  global prepared_data\n",
    "  feature_name = feature_generator.NAME\n",
    "\n",
    "  train_X = np.array(prepared_data[feature_name]['train_X'])\n",
    "  test_X = np.array(prepared_data[feature_name]['test_X'])\n",
    "  if features_bitmap is not None:\n",
    "    train_X = train_X[:, features_bitmap]\n",
    "    test_X = test_X[:, features_bitmap]\n",
    "\n",
    "  classificator.fit(\n",
    "    train_X,\n",
    "    prepared_data[feature_name]['train_Y']\n",
    "  )\n",
    "\n",
    "  test_Y_predicted = classificator.predict(test_X)\n",
    "\n",
    "  precision = precision_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "  recall = recall_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "  f1 = f1_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "  accuracy = accuracy_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "\n",
    "  return {\n",
    "    \"precision\" : round(precision * 100, 2),\n",
    "    \"recall\" : round(recall * 100, 2),\n",
    "    \"f1\" : round(f1 * 100, 2),\n",
    "    \"accuracy\" : round(accuracy * 100, 2),\n",
    "  }\n",
    "\n",
    "class BaselineFeatureGenerator:\n",
    "  NAME = 'Baseline'\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    return np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def base_classification_test(feature_generator, verbose=True, features_bitmap=None, force_feature_update=False):\n",
    "  if force_feature_update:\n",
    "    prepare_data(feature_generator, True, 10000)\n",
    "\n",
    "  classificators = [\n",
    "    {\n",
    "      \"name\": \"SVC\",\n",
    "      \"classificator\": SVC(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"SVC(kernel = 'rbf', random_state = 0)\",\n",
    "      \"classificator\": SVC(kernel = 'rbf', random_state = 0),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"SVC(probability=True)\",\n",
    "      \"classificator\": SVC(probability=True),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"NuSVC\",\n",
    "      \"classificator\": NuSVC(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"LinearSVC\",\n",
    "      \"classificator\": LinearSVC(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"DecisionTreeClassifier\",\n",
    "      \"classificator\": DecisionTreeClassifier(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"DecisionTreeClassifier(criterion='entropy',random_state=0)\",\n",
    "      \"classificator\": DecisionTreeClassifier(criterion=\"entropy\",random_state=0),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"ExtraTreeClassifier\",\n",
    "      \"classificator\": ExtraTreeClassifier(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"KNeighborsClassifier\",\n",
    "      \"classificator\": KNeighborsClassifier(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \" KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\",\n",
    "      \"classificator\":  KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"GaussianNB\",\n",
    "      \"classificator\": GaussianNB(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"BernoulliNB\",\n",
    "      \"classificator\": BernoulliNB(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Perceptron\",\n",
    "      \"classificator\": Perceptron(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"SGDClassifier\",\n",
    "      \"classificator\": SGDClassifier(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"RandomForestClassifier\",\n",
    "      \"classificator\": RandomForestClassifier(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"RandomForestClassifier(n_estimators=1000,criterion='entropy',random_state=0)\",\n",
    "      \"classificator\": RandomForestClassifier(n_estimators=1000,criterion='entropy',random_state=0),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"LogisticRegressionCV\",\n",
    "      \"classificator\": LogisticRegressionCV(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"PassiveAggressiveClassifier\",\n",
    "      \"classificator\": PassiveAggressiveClassifier(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"RidgeClassifierCV\",\n",
    "      \"classificator\": RidgeClassifierCV(),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"LogisticRegression(max_iter = 500000)\",\n",
    "      \"classificator\": LogisticRegression(max_iter = 500000),\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"GradientBoostingClassifier\",\n",
    "      \"classificator\": GradientBoostingClassifier(),\n",
    "    },\n",
    "  ]\n",
    "  res = []\n",
    "  for item in tqdm(classificators):\n",
    "    score = get_metrics(feature_generator, item[\"classificator\"], features_bitmap=features_bitmap)\n",
    "    res.append({\"classificator\": item[\"name\"], \"score\": score})\n",
    "\n",
    "  if verbose:\n",
    "    for r in res:\n",
    "      print(r[\"classificator\"])\n",
    "      print(r[\"score\"])\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_bitmasks(length):\n",
    "  res = []\n",
    "  for i in range(1, 2 ** length):\n",
    "     bitmask = np.zeros(length)\n",
    "     n = int(i)\n",
    "     pos = 0\n",
    "     while n > 0:\n",
    "       bitmask[pos] = n % 2\n",
    "       pos += 1\n",
    "       n = n // 2\n",
    "     bitmask = [b == 1 for b in bitmask]\n",
    "     res.append(bitmask)\n",
    "  return res\n",
    "\n",
    "\n",
    "def feature_selection(feature_generator, top_results=5, verbose=False, bitmask_amount=None):\n",
    "  global prepared_data\n",
    "  feature_name = feature_generator.NAME\n",
    "  assert feature_name in prepared_data\n",
    "  length = len(prepared_data[feature_generator.NAME][\"test_X\"][0])\n",
    "  bitmasks = get_all_bitmasks(length)\n",
    "  results = []\n",
    "  if bitmask_amount is not None:\n",
    "    random.shuffle(bitmasks)\n",
    "    bitmasks = random.sample(bitmasks, bitmask_amount)\n",
    "  for bitmask in tqdm(bitmasks):\n",
    "    if verbose:\n",
    "      print(f\"mask {bitmask}\")\n",
    "    scores = base_classification_test(feature_generator, features_bitmap=bitmask, verbose=False)\n",
    "    for score in scores:\n",
    "      if verbose:\n",
    "        print(f\"classificator {score['classificator']}\")\n",
    "        print(f\"score {score['score']}\")\n",
    "        \n",
    "      results.append({\n",
    "        \"mask\": bitmask, \n",
    "        \"score\": score[\"score\"], \n",
    "        \"classificator\": score[\"classificator\"]\n",
    "      })\n",
    "  \n",
    "  results = sorted(results, key=lambda k: k[\"score\"][\"accuracy\"], reverse=True)\n",
    "  print(\"Top Accuracy\")\n",
    "  for r in results[:top_results]:\n",
    "    print(f\"{r['classificator']} {r['score']} {r['mask']}\")\n",
    "  \n",
    "  results = sorted(results, key=lambda k: k[\"score\"][\"f1\"], reverse=True)\n",
    "  print(\"Top F1\")\n",
    "  for r in results[:top_results]:\n",
    "    print(f\"{r['classificator']} {r['score']} {r['mask']}\")\n",
    "\n",
    "# Uncomment to run feature selection\n",
    "# feature_selection(path_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HungarianGraphFeatureGenerator:\n",
    "  NAME = 'HungarianGraph'\n",
    "\n",
    "  def get_features_for_graphs(self, node_matcher, similarity):\n",
    "    node_matcher.set_threshold(similarity)\n",
    "    g1, g2 = node_matcher.get_converted_graphs()\n",
    "    score_normalized = compare_graphs(g1, g2, False, True)\n",
    "    score_raw = compare_graphs(g1, g2, False, False)\n",
    "    return np.array([score_normalized, score_raw])\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = get_dependancy_graph(s1, False)\n",
    "    g2 = get_dependancy_graph(s2, False)\n",
    "    node_matcher = HungarianGraphNodesMatcher(g1, g2, 0.9)\n",
    "    \n",
    "    features = np.array([])\n",
    "    \n",
    "    for similarity in [0.8, 0.85,  0.90, 0.95]:\n",
    "      features = np.append(features, self.get_features_for_graphs(node_matcher, similarity))\n",
    "    \n",
    "    return features\n",
    "\n",
    "hungarian_feature_generator = HungarianGraphFeatureGenerator()\n",
    "\n",
    "# It gives {'precision': 73.99, 'recall': 89.54, 'f1': 81.03, 'accuracy': 72.12}\n",
    "# on LinearSVC()\n",
    "# LinearSVC {'precision': 71.27, 'recall': 95.82, 'f1': 81.74, 'accuracy': 71.54} \n",
    "# [False, False, False, False, False, True, True, True]\n",
    "# LinearSVC {'precision': 71.3, 'recall': 95.73, 'f1': 81.73, 'accuracy': 71.54} \n",
    "# [True, False, False, True, False, False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# verbose_data(hungarian_feature_generator)\n",
    "# base_classification_test(hungarian_feature_generator, force_feature_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature_selection(hungarian_feature_generator, verbose=False, bitmask_amount=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature_selection(hungarian_feature_generator, verbose=False, bitmask_amount=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test\n",
    "verbose_data(hungarian_feature_generator)\n",
    "\n",
    "# prepare_data(hungarian_feature_generator, True, 100000)\n",
    "# base_classification_test(hungarian_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HungarianNodeFeatureGenerator:\n",
    "  NAME = 'HungarianNode'\n",
    "\n",
    "  def get_features_for_graphs(self, node_matcher, similarity):\n",
    "    node_matcher.set_threshold(similarity)\n",
    "    g1, g2 = node_matcher.get_converted_graphs()\n",
    "    n1, n2 = len(g1), len(g2)\n",
    "    num_matched_nodes = len(node_matcher.graph1_to_graph2)\n",
    "    percent_matched = num_matched_nodes * 2. / (n1 + n2)\n",
    "    features = np.array([n1, n2, percent_matched])\n",
    "    return features\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = get_dependancy_graph(s1, False)\n",
    "    g2 = get_dependancy_graph(s2, False)\n",
    "    node_matcher = HungarianGraphNodesMatcher(g1, g2, 0.9)\n",
    "    \n",
    "    features = np.array([])\n",
    "    \n",
    "    for similarity in [0.8, 0.85,  0.90, 0.95]:\n",
    "      features = np.append(features, self.get_features_for_graphs(node_matcher, similarity))\n",
    "    \n",
    "    return features\n",
    "\n",
    "#   def get_features(self, s1, s2):\n",
    "#     graph1 = get_dependancy_graph(s1, False)\n",
    "#     graph2 = get_dependancy_graph(s2, False)\n",
    "#     node_matcher = HungarianGraphNodesMatcher(graph1, graph2, 0.9)\n",
    "    \n",
    "#     g1, g2 = node_matcher.get_converted_graphs()\n",
    "#     n1, n2 = len(g1), len(g2)\n",
    "#     num_matched_nodes = len(node_matcher.graph1_to_graph2)\n",
    "#     percent_matched = num_matched_nodes * 2. / (n1 + n2)\n",
    "#     features = np.array([n1, n2, percent_matched])\n",
    "#     return features\n",
    "\n",
    "hungarian_node_feature_generator = HungarianNodeFeatureGenerator()\n",
    "# It gives {'precision': 74.5, 'recall': 90.67, 'f1': 81.79, 'accuracy': 73.16}\n",
    "# on SGDClassifier(),\n",
    "\n",
    "\n",
    "# LinearSVC {'precision': 73.62, 'recall': 92.94, 'f1': 82.16, 'accuracy': 73.16} \n",
    "# [False, False, False, \n",
    "# True, False, True, \n",
    "# False, True, False,\n",
    "# True, False, True]\n",
    "# Perceptron {'precision': 72.96, 'recall': 94.33, 'f1': 82.28, 'accuracy': 72.99} [True, False, True, False, False, True, True, False, False, True, False, True]\n",
    "# LinearSVC {'precision': 73.52, 'recall': 93.2, 'f1': 82.2, 'accuracy': 73.16} \n",
    "# [False, True, False, \n",
    "# False, False, True, \n",
    "# False, False, False, \n",
    "# False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose_data(hungarian_node_feature_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test\n",
    "# prepare_data(hungarian_node_feature_generator, True, 100000)\n",
    "# base_classification_test(hungarian_node_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphTraversal:\n",
    "  def __init__(self, graph=None, sentance=None):\n",
    "    assert graph is not None or sentance is not None\n",
    "    if graph is not None:\n",
    "      self.g = graph\n",
    "    else:\n",
    "      self.g = GraphBuilder.build_nx_graph_from_sentance(sentance)\n",
    "  \n",
    "  def get_paths_from_root_to_leafs(self, root=0):\n",
    "     #                 node. parent. path.      \n",
    "    res, stack = [], [(root, None, [])]\n",
    "    while stack:\n",
    "        node, parent, path = stack.pop()\n",
    "        path.append(node)\n",
    "        neighbours = [n for n, _ in self.g.adj[node].items()]\n",
    "        if len(neighbours) == 1 and neighbours[0] == parent:\n",
    "            res.append(path)\n",
    "        for n in neighbours:\n",
    "          if n == parent:\n",
    "            continue\n",
    "          stack.append((n, node, path[:]))\n",
    "    return res\n",
    "    \n",
    "  def get_all_paths_with_len(self, root=0, length=0):\n",
    "    \"\"\"\n",
    "    Return list of pathes with specificified len + 1.\n",
    "    The start is every node.\n",
    "    \n",
    "    For the tree:\n",
    "           1\n",
    "         2   3\n",
    "       5\n",
    "         6\n",
    "         \n",
    "    Len = 2:\n",
    "    [1, 2, 5]\n",
    "    [2, 5, 6]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #                  node. parent. path.                  \n",
    "    res, stack = [], [(root, None, [])]\n",
    "    started_new_path = {root}\n",
    "    while stack:\n",
    "        node, parent, path = stack.pop()\n",
    "        path.append(node)\n",
    "        neighbours = [n for n, _ in self.g.adj[node].items()]\n",
    "        if len(path) == length + 1:\n",
    "          res.append(path)\n",
    "        for n in neighbours:\n",
    "          if n == parent:\n",
    "            continue\n",
    "          if len(path) < length + 1:\n",
    "            stack.append((n, node, path[:]))\n",
    "          if n not in started_new_path:\n",
    "            stack.append((n, node, []))\n",
    "            started_new_path.add(n)\n",
    "        \n",
    "    return res\n",
    "\n",
    "  def get_all_subtrees_with_depth(self, root=0, parent=None, length=0):\n",
    "    \"\"\"\n",
    "      Return array of subtrees.\n",
    "      Each subtree is defined by indexes of their nodes.\n",
    "    \"\"\"\n",
    "    #                  node. parent. distance.  \n",
    "    res, stack = [], [(root, parent, 0)]\n",
    "    reached_depth = False\n",
    "    while stack:\n",
    "        node, _parent, distance = stack.pop()\n",
    "        res.append(node)\n",
    "        if distance >= length:\n",
    "          reached_depth = True\n",
    "          continue\n",
    "        neighbours = [n for n, _ in self.g.adj[node].items()]\n",
    "        for n in neighbours:\n",
    "          if n == _parent:\n",
    "            continue\n",
    "          stack.append((n, node, distance + 1))\n",
    "      \n",
    "    all_subtrees = []\n",
    "    if reached_depth:\n",
    "      all_subtrees.append(res)\n",
    "    for n, _ in self.g.adj[root].items():\n",
    "      if n == parent:\n",
    "        continue\n",
    "      all_subtrees += self.get_all_subtrees_with_depth(n, root, length)\n",
    "\n",
    "    return all_subtrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_graphs(graphs, word_labels=False):\n",
    "  for g in graphs:\n",
    "     fig, ax = plt.subplots(1, 1)\n",
    "     if word_labels:\n",
    "       nx.draw(g, arrowstyle=\"->\", with_labels=True, pos=nx.kamada_kawai_layout(g), ax=ax, labels={n: g.nodes[n]['node'] for n in g.nodes})\n",
    "     else:\n",
    "       nx.draw(g, with_labels=True, pos=nx.kamada_kawai_layout(g), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphFeatures:\n",
    "  def __init__(self, graph=None, sentance=None):\n",
    "    assert graph is not None or sentance is not None\n",
    "    if graph is not None:\n",
    "      self.g = graph\n",
    "    else:\n",
    "      self.g = GraphBuilder.build_nx_graph_from_sentance(sentance)\n",
    "  \n",
    "  def get_path_features(self, length=0):\n",
    "    traversal = GraphTraversal(graph=self.g)\n",
    "    pathes = traversal.get_all_paths_with_len(length=length)\n",
    "    \n",
    "    pathes_with_nodes = [\n",
    "      [self.g.nodes[node] for node in path ]\n",
    "      for path in pathes \n",
    "    ]\n",
    "\n",
    "    filtered_pathes_with_nodes = [\n",
    "      path \n",
    "      for path in pathes_with_nodes \n",
    "      if all(\n",
    "          node[\"token\"] is not None and node[\"token\"].has_vector \n",
    "          for node in path\n",
    "      )\n",
    "    ]\n",
    "\n",
    "    aggregated_vectors = [\n",
    "       sum([node[\"token\"].vector for node in path])\n",
    "       for path in filtered_pathes_with_nodes\n",
    "    ]\n",
    "\n",
    "    return aggregated_vectors\n",
    "\n",
    "  def get_subtree_features(self, length=0, remove_tree_without_vector=True, remove_stop_words=False, idf_model=None):\n",
    "    \"\"\"\n",
    "    Return list of vectors, where each vector represent one subtree.\n",
    "    Subtree is created by aggregating vectors in this subtree.\n",
    "\n",
    "    Keyword arguments:\n",
    "    length -- the real part (default 0.0)\n",
    "    remove_tree_without_vector -- remove whole tree if at least one vector inside it \n",
    "      is empty (non common word)\n",
    "    remove_stop_words - remove word from tree if it is stop word\n",
    "    idf_model - If present, multiply vector by word idf\n",
    "    \"\"\"\n",
    "    traversal = GraphTraversal(graph=self.g)\n",
    "    subtrees = traversal.get_all_subtrees_with_depth(length=length)\n",
    "    \n",
    "    subtrees_with_nodes = [\n",
    "      [self.g.nodes[node] for node in subtree ]\n",
    "      for subtree in subtrees\n",
    "    ]\n",
    "\n",
    "    if remove_tree_without_vector:\n",
    "      subtrees_with_nodes = [\n",
    "        subtree \n",
    "        for subtree in subtrees_with_nodes \n",
    "        if all(\n",
    "            node[\"token\"] is not None and node[\"token\"].has_vector \n",
    "            for node in subtree\n",
    "        )\n",
    "      ]\n",
    "\n",
    "    idf = lambda word: 1\n",
    "    \n",
    "    if idf_model is not None:\n",
    "        idf = lambda word: idf_model.get_idf(word)\n",
    "    \n",
    "    aggregated_vectors = [\n",
    "       sum([\n",
    "          node[\"token\"].vector * idf(node[\"token\"])\n",
    "          for node in subtree \n",
    "          # If remove_tree_without_vector == false\n",
    "          if node[\"token\"] is not None and node[\"token\"].has_vector\n",
    "          and (not remove_stop_words or not node[\"token\"].is_stop)\n",
    "        ])\n",
    "       for subtree in subtrees_with_nodes\n",
    "    ]\n",
    "\n",
    "    # Filter empty vectors\n",
    "    aggregated_vectors = [\n",
    "      v\n",
    "      for v in aggregated_vectors\n",
    "      if not np.isscalar(v)\n",
    "    ]\n",
    "\n",
    "    return aggregated_vectors\n",
    "\n",
    "  def get_simple_edge_features(self):\n",
    "    \"\"\"\n",
    "    Return list of edges\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for (start_idx, end_idx, dependancy_type) in self.g.edges.data('dependancy_type'):\n",
    "        item = {}\n",
    "        item['start_idx'] = start_idx\n",
    "        item['end_idx'] = end_idx\n",
    "        item['dependancy_type'] = dependancy_type\n",
    "        item['start_node'] = self.g.nodes[start_idx]\n",
    "        item['end_node'] = self.g.nodes[end_idx]\n",
    "        edges.append(item)\n",
    "        \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vector:\n",
    "  @classmethod\n",
    "  def get_norm(cls, v):\n",
    "    total = (v ** 2).sum()\n",
    "    return np.sqrt(total) if total != 0 else 0\n",
    "\n",
    "  @classmethod\n",
    "  def similarity(cls, v1, v2):\n",
    "    v1_norm = cls.get_norm(v1)\n",
    "    v2_norm = cls.get_norm(v2)\n",
    "    if v1_norm == 0 or v1_norm == 0:\n",
    "      return 0.0\n",
    "    return (np.dot(v1, v2) / (v1_norm * v2_norm))\n",
    "\n",
    "class MatchFeatureVectors:\n",
    "  @classmethod\n",
    "  def match_feature_vectors(cls, features1, features2, similarity=0.8):\n",
    "    \"\"\"\n",
    "      This function tries to do the following:\n",
    "      1) For each vector in features1 try to find whether vector with good similarity exist in features2.\n",
    "      Return ammount of matched vectors.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    features1_norm = []\n",
    "    for v1 in features1:\n",
    "      for v2 in features2:\n",
    "        score = Vector.similarity(v1, v2)\n",
    "        if score > similarity:\n",
    "          count += 1\n",
    "          break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nxv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = get_sample(0)\n",
    "g1 = GraphBuilder.build_nx_graph_from_sentance(sample['s1'])\n",
    "g2 = GraphBuilder.build_nx_graph_from_sentance(sample['s2'])\n",
    "g_f1 = GraphFeatures(g1)\n",
    "g_f2 = GraphFeatures(g2)\n",
    "f1 = g_f1.get_subtree_features(length=1)\n",
    "f2 = g_f2.get_subtree_features(length=1)\n",
    "score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity=0.8)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = get_sample(0)\n",
    "g1 = GraphBuilder.build_nx_graph_from_sentance(sample['s1'])\n",
    "g2 = GraphBuilder.build_nx_graph_from_sentance(sample['s2'])\n",
    "g_f1 = GraphFeatures(g1)\n",
    "g_f2 = GraphFeatures(g2)\n",
    "f1 = g_f1.get_path_features(length=3)\n",
    "f2 = g_f2.get_path_features(length=3)\n",
    "score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity=0.8)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PathFeatureGenerator:\n",
    "  NAME = 'PathSimilarity'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def get_feature_for_length(self, g_f1, g_f2, length):\n",
    "    f1 = g_f1.get_path_features(length=length)\n",
    "    f2 = g_f2.get_path_features(length=length)\n",
    "    \n",
    "    score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity=self.SIMILARITY)\n",
    "\n",
    "    norm = len(f1) + len(f2)\n",
    "    return (score * 2.) / norm if norm != 0 else 0\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.get_feature_for_length(g_f1, g_f2, 0),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 1),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 2),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 3),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 4),\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "path_feature_generator = PathFeatureGenerator()\n",
    "# SGDClassifier {'precision': 74.34, 'recall': 90.41, 'f1': 81.59, 'accuracy': 72.87} [True, False, True, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose_data(path_feature_generator, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It seems that PathFeatureGenerator with more length are giving more stable (core) feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test\n",
    "# prepare_data(path_feature_generator, True)\n",
    "# base_classification_test(path_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Give it back! He pleaded.\")\n",
    "token = doc[0]\n",
    "vector = token.vector\n",
    "total = (vector ** 2).sum()\n",
    "norm = np.sqrt(total) if total != 0 else 0\n",
    "print(norm)\n",
    "print(token.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = get_sample(0)\n",
    "g1 = GraphBuilder.build_nx_graph_from_sentance(sample['s1'])\n",
    "g2 = GraphBuilder.build_nx_graph_from_sentance(sample['s2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_graphs([g1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g1.nodes[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleGraphVector:\n",
    "  @classmethod\n",
    "  def get_graph_vector(cls, g):\n",
    "    \"\"\"\n",
    "    g: networkx graph\n",
    "    Return vector\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for node_index in g.nodes:\n",
    "      node = g.nodes[node_index]\n",
    "      if node['token'] is not None and node['token'].has_vector:\n",
    "        attention = g.degree[node_index]\n",
    "        vectors.append(attention * np.array(node['token'].vector))\n",
    "    return sum(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleGraphVectorFeatureGenerator:\n",
    "  \"\"\"\n",
    "  Compute only one feature - one vector of the whole tree\n",
    "  \"\"\"\n",
    "  NAME = 'SimpleGraphVectorFeatureGenerator'\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    v1 = SimpleGraphVector.get_graph_vector(g1)\n",
    "    v2 = SimpleGraphVector.get_graph_vector(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      Vector.similarity(v1, v2)\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "simple_graph_vector_feature_generator = SimpleGraphVectorFeatureGenerator()\n",
    "# GaussianNB {'precision': 71.35, 'recall': 92.5, 'f1': 80.56, 'accuracy': 70.32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test\n",
    "# prepare_data(simple_graph_vector_feature_generator, True, 10000)\n",
    "# base_classification_test(simple_graph_vector_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose_data(simple_graph_vector_feature_generator, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubtreeFeatureGenerator:\n",
    "  NAME = 'SubtreeFeature'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def get_feature_for_length(self, g_f1, g_f2, length):\n",
    "    f1 = g_f1.get_subtree_features(length=length)\n",
    "    f2 = g_f2.get_subtree_features(length=length)\n",
    "    \n",
    "    score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity=self.SIMILARITY)\n",
    "\n",
    "    norm = len(f1) + len(f2)\n",
    "    return (score * 2.) / norm if norm != 0 else 0\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.get_feature_for_length(g_f1, g_f2, 0),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 1),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 2),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 3),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 4),\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "subtree_feature_generator = SubtreeFeatureGenerator()\n",
    "# LinearSVC {'precision': 74.85, 'recall': 89.8, 'f1': 81.65, 'accuracy': 73.16}\n",
    "# SGDClassifier {'precision': 74.84, 'recall': 90.5, 'f1': 81.93, 'accuracy': 73.45}\n",
    "# LogisticRegression(max_iter = 500000) {'precision': 75.02, 'recall': 90.32, 'f1': 81.96, 'accuracy': 73.57} [True, True, True, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test\n",
    "# prepare_data(subtree_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(subtree_feature_generator)\n",
    "# Uncomment to run feature selection\n",
    "# feature_selection(subtree_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose_data(subtree_feature_generator, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RootNodeFeatureGenerator:\n",
    "  NAME = 'RootNodeFeature'\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "\n",
    "    root_node1 = GraphBuilder.get_root_node(g1)\n",
    "    root_node2 = GraphBuilder.get_root_node(g2)\n",
    "\n",
    "    if root_node1['token'].has_vector and root_node2['token'].has_vector:\n",
    "      score = root_node1['token'].similarity(root_node2['token'])\n",
    "    else:\n",
    "      score = 0\n",
    "\n",
    "    features = np.array([\n",
    "      score,\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "root_node_feature_generator = RootNodeFeatureGenerator()\n",
    "# SVC {'precision': 66.49, 'recall': 100.0, 'f1': 79.87, 'accuracy': 66.49}\n",
    "# By itself is not usefull, but maybe in combination could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare_data(root_node_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(root_node_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class GraphAlgoFeatureGenerator:\n",
    "#   NAME = 'GraphAlgoFeature'\n",
    "\n",
    "#   def get_features(self, s1, s2):\n",
    "#     g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "#     g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "\n",
    "#     s1 = networkx_algorithms.smetric.s_metric(g1, normalized=False)\n",
    "#     s2 = networkx_algorithms.smetric.s_metric(g2, normalized=False)\n",
    "    \n",
    "#     w1 = networkx_algorithms.wiener.wiener_index(g1)\n",
    "#     w2 = networkx_algorithms.wiener.wiener_index(g2)\n",
    "\n",
    "#     # r1 = networkx_algorithms.richclub.rich_club_coefficient(g1)\n",
    "#     # r2 = networkx_algorithms.richclub.rich_club_coefficient(g2)\n",
    "\n",
    "#     features = np.array([\n",
    "#       s1 / (s1 + s2),\n",
    "#       s2 / (s1 + s2),\n",
    "#       abs(s1 - s2) / (s1 + s2),\n",
    "#       w1 / (w1 + w2),\n",
    "#       w2 / (w1 + w2),\n",
    "#       abs(w1 - w2) / (w1 + w2),\n",
    "#       # abs(r1[0] - r2[0]),\n",
    "#       # abs(r1[1] - r2[1]),\n",
    "#       # abs(r1[2] - r2[2]),      \n",
    "#     ])\n",
    "\n",
    "#     return features\n",
    "\n",
    "# graph_algo_feature_generator = GraphAlgoFeatureGenerator()\n",
    "# Even after feature selection\n",
    "# PassiveAggressiveClassifier {'precision': 68.34, 'recall': 95.2, 'f1': 79.56, 'accuracy': 67.48} [False, False, False, True, True, True]\n",
    "# So it's not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare_data(graph_algo_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(graph_algo_feature_generator)\n",
    "# feature_selection(graph_algo_feature_generator, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import json\n",
    "# from json import JSONEncoder\n",
    "\n",
    "# class NumpyArrayEncoder(JSONEncoder):\n",
    "#     def default(self, obj):\n",
    "#         if isinstance(obj, np.ndarray):\n",
    "#             return obj.tolist()\n",
    "#         return JSONEncoder.default(self, obj)\n",
    "\n",
    "# with open('/content/drive/My Drive/phd/prepared_data.json', 'w') as f:\n",
    "#   json.dump(prepared_data, f, cls=NumpyArrayEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Try to remove stop words (don't use their vectors):\n",
    " - Results:\n",
    "    SubtreeFeature - not very usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubtreeFeatureGeneratorWithoutStopWords:\n",
    "  NAME = 'SubtreeFeatureWithoutStopWords'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def get_feature_for_length(self, g_f1, g_f2, length):\n",
    "    f1 = g_f1.get_subtree_features(length=length, remove_stop_words=True)\n",
    "    f2 = g_f2.get_subtree_features(length=length, remove_stop_words=True)\n",
    "    \n",
    "    score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity=self.SIMILARITY)\n",
    "\n",
    "    norm = len(f1) + len(f2)\n",
    "    return (score * 2.) / norm if norm != 0 else 0\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.get_feature_for_length(g_f1, g_f2, 0),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 1),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 2),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 3),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 4),\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "subtree_without_stop_words_feature_generator = SubtreeFeatureGeneratorWithoutStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare_data(subtree_without_stop_words_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(subtree_without_stop_words_feature_generator)\n",
    "# feature_selection(subtree_without_stop_words_feature_generator, verbose=False)\n",
    "\n",
    "# Results\n",
    "# SVC {'precision': 72.9, 'recall': 91.46, 'f1': 81.13, 'accuracy': 71.71}\n",
    "# feature selection didn't improve results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AllFeatureGenerator:\n",
    "  NAME = 'AllFeatures'\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    generators = [\n",
    "        HungarianGraphFeatureGenerator(),\n",
    "        HungarianNodeFeatureGenerator(),\n",
    "        PathFeatureGenerator(),\n",
    "        SubtreeFeatureGenerator(),\n",
    "    ]\n",
    "    features = np.array([])\n",
    "    for generator in generators:\n",
    "      features = np.append(features, generator.get_features(s1, s2))\n",
    "    return features\n",
    "\n",
    "all_feature_generator = AllFeatureGenerator()\n",
    "# LogisticRegression(max_iter = 500000) {'precision': 76.04, 'recall': 88.84, 'f1': 81.95, 'accuracy': 73.97}\n",
    "# RandomForestClassifier(n_estimators=1000,criterion='entropy',random_state=0) {'precision': 76.32, 'recall': 88.23, 'f1': 81.84, 'accuracy': 73.97}\n",
    "\n",
    "# RidgeClassifierCV {'precision': 76.28, 'recall': 89.71, 'f1': 82.45, 'accuracy': 74.61} [True, False, True, False, False, True, False, False, True, False, True, True, False, True, False]\n",
    "# LogisticRegressionCV {'precision': 76.8, 'recall': 88.58, 'f1': 82.27, 'accuracy': 74.61} [True, False, True, True, True, False, False, True, True, True, False, False, True, True, False]\n",
    "# RidgeClassifierCV {'precision': 76.09, 'recall': 90.15, 'f1': 82.52, 'accuracy': 74.61} [True, True, False, False, False, True, True, False, False, False, True, True, False, False, True]\n",
    "# LogisticRegression(max_iter = 500000) {'precision': 76.4, 'recall': 89.45, 'f1': 82.41, 'accuracy': 74.61} [True, False, True, True, True, True, False, True, True, True, False, False, False, True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare_data(all_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(all_feature_generator)\n",
    "# all_feature_generator = AllFeatureGenerator()\n",
    "# feature_selection(all_feature_generator, verbose=True, bitmask_amount=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AllFeatureGeneratorV1:\n",
    "  NAME = 'AllFeatureV1'\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    generators = [\n",
    "        HungarianGraphFeatureGenerator(),\n",
    "        HungarianNodeFeatureGenerator(),\n",
    "        PathFeatureGenerator(),\n",
    "        SubtreeFeatureGenerator(),\n",
    "        SubtreeFeatureGeneratorWithoutStopWords(),\n",
    "    ]\n",
    "    features = np.array([])\n",
    "    for generator in generators:\n",
    "      features = np.append(features, generator.get_features(s1, s2))\n",
    "    return features\n",
    "\n",
    "all_feature_generator_v1 = AllFeatureGeneratorV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare_data(all_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(all_feature_generator)\n",
    "# all_feature_generator = AllFeatureGenerator()\n",
    "# feature_selection(all_feature_generator, verbose=False, bitmask_amount=100)\n",
    "\n",
    "# Top Accuracy\n",
    "# LinearSVC {'precision': 76.62, 'recall': 89.45, 'f1': 82.54, 'accuracy': 74.84} [True, False, False, False, False, False, False, False, True, True, True, True, True, True, False]\n",
    "# RidgeClassifierCV {'precision': 76.57, 'recall': 89.45, 'f1': 82.51, 'accuracy': 74.78} [True, False, False, False, False, False, False, False, True, True, True, True, True, True, False]\n",
    "# LogisticRegression(max_iter = 500000) {'precision': 76.79, 'recall': 88.84, 'f1': 82.38, 'accuracy': 74.72} [False, True, False, True, True, True, False, False, False, True, False, True, False, False, True]\n",
    "# LogisticRegression(max_iter = 500000) {'precision': 76.59, 'recall': 89.28, 'f1': 82.45, 'accuracy': 74.72} [False, False, True, True, True, False, False, True, False, False, True, False, True, True, False]\n",
    "# RidgeClassifierCV {'precision': 76.83, 'recall': 88.75, 'f1': 82.36, 'accuracy': 74.72} [False, False, False, False, True, True, False, True, False, False, False, True, True, True, False]\n",
    "# Top F1\n",
    "# LogisticRegression(max_iter = 500000) {'precision': 76.01, 'recall': 90.32, 'f1': 82.55, 'accuracy': 74.61} [True, False, True, False, False, True, True, True, False, True, False, False, False, True, True]\n",
    "# LinearSVC {'precision': 76.62, 'recall': 89.45, 'f1': 82.54, 'accuracy': 74.84} [True, False, False, False, False, False, False, False, True, True, True, True, True, True, False]\n",
    "# RidgeClassifierCV {'precision': 76.57, 'recall': 89.45, 'f1': 82.51, 'accuracy': 74.78} [True, False, False, False, False, False, False, False, True, True, True, True, True, True, False]\n",
    "# SGDClassifier {'precision': 75.22, 'recall': 91.28, 'f1': 82.47, 'accuracy': 74.2} [False, True, True, False, True, False, False, False, False, False, True, True, False, True, False]\n",
    "# Perceptron {'precision': 75.97, 'recall': 90.15, 'f1': 82.46, 'accuracy': 74.49} [True, False, True, False, True, False, False, False, False, False, True, True, True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AnalyzePredictions:\n",
    "  CORRECT_PREDICTIONS = 'CORRECT_PREDICTIONS'\n",
    "  WRONG_PREDICTIONS = 'WRONG_PREDICTIONS'\n",
    "  \n",
    "  @classmethod\n",
    "  def get_predictions(cls, feature_generator, classificator, mode, verbose=True, limit=10):\n",
    "    feature_name = feature_generator.NAME\n",
    "    assert feature_name in prepared_data, \"No features found\"\n",
    "\n",
    "    test_data = DataGenerator.get_test_data()\n",
    "    \n",
    "    train_X = prepared_data[feature_name]['train_X']\n",
    "    test_X = prepared_data[feature_name]['test_X']\n",
    "    train_Y = prepared_data[feature_name]['train_Y']\n",
    "    test_Y = prepared_data[feature_name]['test_Y']\n",
    "\n",
    "\n",
    "    classificator.fit(train_X, train_Y)\n",
    "\n",
    "    test_Y_predicted = classificator.predict(test_X)\n",
    "\n",
    "    res = []\n",
    "    for data, features, prediction, label in zip(test_data, test_X, test_Y_predicted, test_Y):\n",
    "      if (mode == AnalyzePredictions.CORRECT_PREDICTIONS and prediction == label or \n",
    "         mode == AnalyzePredictions.WRONG_PREDICTIONS and prediction != label):\n",
    "          res.append({\n",
    "              \"raw\": data,\n",
    "              \"features\": features,\n",
    "              \"prediction\": prediction,\n",
    "              \"label\": label\n",
    "          })\n",
    "          if verbose:\n",
    "            print(data)\n",
    "            print(features)\n",
    "            print(f\"Prediction {prediction}, but label {label}\")\n",
    "          limit -= 1\n",
    "          if limit == 0:\n",
    "            break\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_sample(p):\n",
    "  print (p[\"raw\"][\"s1\"])\n",
    "  print (p[\"raw\"][\"s2\"])\n",
    "  print (f\"Label {p['raw']['label']}\")\n",
    "  print (f\"Prediction {p['prediction']}\")\n",
    "  print (p[\"features\"])\n",
    "\n",
    "# model = LinearSVC()\n",
    "# prepare_data(subtree_feature_generator, True, 10000)\n",
    "# not_predicted = AnalyzePredictions.get_predictions(subtree_feature_generator, model, AnalyzePredictions.WRONG_PREDICTIONS,  verbose=False)\n",
    "# ok_predicted = AnalyzePredictions.get_predictions(subtree_feature_generator, model, AnalyzePredictions.CORRECT_PREDICTIONS,  verbose=False, limit=10)\n",
    "# sample_wrong = not_predicted[0]\n",
    "# sample_ok = ok_predicted[0]\n",
    "\n",
    "# print_sample(sample_wrong)\n",
    "# print_sample(sample_ok)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Starting to work with edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleEdgeMatcher:\n",
    "  NAME = 'SimpleEdgeMatcher'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def simple_match_edges(self, g_f1, g_f2):\n",
    "    f1 = g_f1.get_simple_edge_features()\n",
    "    f2 = g_f2.get_simple_edge_features()\n",
    "    \n",
    "    score = 0\n",
    "    for edge1 in f1:\n",
    "        if (edge1['start_node']['token'] is None or \n",
    "            not edge1['start_node']['token'].has_vector\n",
    "            or edge1['end_node']['token'] is None\n",
    "            or not edge1['end_node']['token'].has_vector):\n",
    "                continue\n",
    "        for edge2 in f2:\n",
    "            if (edge2['start_node']['token'] is None \n",
    "            or not edge2['start_node']['token'].has_vector\n",
    "            or edge2['end_node']['token'] is None\n",
    "            or not edge2['end_node']['token'].has_vector):\n",
    "                continue\n",
    "            if (Vector.similarity(\n",
    "                    edge1['start_node']['token'].vector, \n",
    "                    edge2['start_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "                and Vector.similarity(\n",
    "                    edge1['end_node']['token'].vector, \n",
    "                    edge2['end_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "               ):\n",
    "                score += 1\n",
    "        \n",
    "    similarity_score = (1. * score) / (len(f1) * len(f2))\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.simple_match_edges(g_f1, g_f2)\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "simple_edge_matcher_feature_generator = SimpleEdgeMatcher()\n",
    "# Don't efective \n",
    "# GaussianNB {'precision': 67.78, 'recall': 97.56, 'f1': 79.99, 'accuracy': 67.54}\n",
    "# RidgeClassifierCV {'precision': 66.82, 'recall': 99.74, 'f1': 80.03, 'accuracy': 66.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare_data(simple_edge_matcher_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(simple_edge_matcher_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleEdgeMatcherWithDependancy:\n",
    "  NAME = 'SimpleEdgeMatcherWithDependancy'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def simple_match_edges_with_dependancy_type(self, g_f1, g_f2):\n",
    "    f1 = g_f1.get_simple_edge_features()\n",
    "    f2 = g_f2.get_simple_edge_features()\n",
    "    \n",
    "    score = 0\n",
    "    total = 0\n",
    "    for edge1 in f1:\n",
    "        if (edge1['start_node']['token'] is None or \n",
    "            not edge1['start_node']['token'].has_vector\n",
    "            or edge1['end_node']['token'] is None\n",
    "            or not edge1['end_node']['token'].has_vector):\n",
    "                continue\n",
    "        for edge2 in f2:\n",
    "            if (edge2['start_node']['token'] is None \n",
    "            or not edge2['start_node']['token'].has_vector\n",
    "            or edge2['end_node']['token'] is None\n",
    "            or not edge2['end_node']['token'].has_vector):\n",
    "                continue\n",
    "            if (Vector.similarity(\n",
    "                    edge1['start_node']['token'].vector, \n",
    "                    edge2['start_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "                and Vector.similarity(\n",
    "                    edge1['end_node']['token'].vector, \n",
    "                    edge2['end_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "               ):\n",
    "                if (edge1['dependancy_type'] == edge2['dependancy_type']):\n",
    "                    score += 1\n",
    "                total += 1\n",
    "        \n",
    "    similarity_score = 0 if total == 0 else (1. * score) / total\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.simple_match_edges_with_dependancy_type(g_f1, g_f2)\n",
    "    ])\n",
    "    \n",
    "    simple_edge_matcher_feature_generator = SimpleEdgeMatcher()\n",
    "    features = np.append(features, simple_edge_matcher_feature_generator.get_features(s1, s2))\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "simple_edge_matcher_with_dependancy_feature_generator = SimpleEdgeMatcherWithDependancy()\n",
    "# GradientBoostingClassifier {'precision': 68.77, 'recall': 94.25, 'f1': 79.51, 'accuracy': 67.71}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare_data(simple_edge_matcher_with_dependancy_feature_generator, True, 10000)\n",
    "# scores = base_classification_test(simple_edge_matcher_with_dependancy_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleApproximateBigramKernel:\n",
    "  \"\"\"\n",
    "        From https://www.aclweb.org/anthology/L16-1452.pdf\n",
    "        Simple Approximate Bigram Kernel (SABK)\n",
    "  \"\"\"\n",
    "\n",
    "  NAME = 'SimpleApproximateBigramKernel'\n",
    "  EDGE_SIMILARITY_SCORE = 2\n",
    "\n",
    "  @classmethod  \n",
    "  def node_similarity(cls, node1, node2):\n",
    "    if (node1['token'] is None or \n",
    "        node2['token'] is None or \n",
    "        not node1['token'].has_vector or \n",
    "        not node2['token'].has_vector\n",
    "       ):\n",
    "        return 1 if node1['node'] == node2['node'] else 0\n",
    "    else:\n",
    "        return Vector.similarity(\n",
    "            node1['token'].vector, \n",
    "            node2['token'].vector\n",
    "        )\n",
    "  @classmethod  \n",
    "  def edge_similarity(cls, edge1, edge2):\n",
    "    return SimpleApproximateBigramKernel.EDGE_SIMILARITY_SCORE if edge1['dependancy_type'] == edge2['dependancy_type'] else 1\n",
    "\n",
    "  @classmethod  \n",
    "  def similarity(cls, edge1, edge2):\n",
    "    start_node_similarity = cls.node_similarity(edge1['start_node'], edge2['start_node'])\n",
    "    end_node_similarity = cls.node_similarity(edge1['end_node'], edge2['end_node'])\n",
    "    \n",
    "    edge_similarity = cls.edge_similarity(edge1, edge2)\n",
    "    \n",
    "    return (start_node_similarity + end_node_similarity) * edge_similarity\n",
    "    \n",
    "  @classmethod  \n",
    "  def compute_simple_approximate_bigram_kernel(cls, g_f1, g_f2):\n",
    "    f1 = g_f1.get_simple_edge_features()\n",
    "    f2 = g_f2.get_simple_edge_features()\n",
    "    \n",
    "    similarity_score = 0\n",
    "\n",
    "    for edge1 in f1:\n",
    "        for edge2 in f2:\n",
    "            similarity_score += cls.similarity(edge1, edge2)\n",
    "        \n",
    "    similarity_score = (similarity_score * 1.) / (len(g1.nodes) + len(g2.nodes))\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      SimpleApproximateBigramKernel.compute_simple_approximate_bigram_kernel(g_f1, g_f2)\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "simple_approximate_bigram_kernel = SimpleApproximateBigramKernel()\n",
    "# LogisticRegression(max_iter = 500000) {'precision': 67.65, 'recall': 98.08, 'f1': 80.07, 'accuracy': 67.54}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scores = base_classification_test(simple_approximate_bigram_kernel, force_feature_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TfIdf:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.prepare_corpus()\n",
    "        self.fit()\n",
    "        \n",
    "    def prepare_corpus(self):\n",
    "        self.corpus = [x['s1'] for x in self.data] + [x['s2'] for x in self.data]\n",
    "        self.corpus = list(set(self.corpus))\n",
    "        self.corpus = sorted(self.corpus)\n",
    "        self.corpus_len = len(self.corpus)\n",
    "                \n",
    "        self.sent_to_index = {}\n",
    "        for index, s in enumerate(self.corpus):\n",
    "            self.sent_to_index[s] = index\n",
    "            \n",
    "    def fit(self):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        self.words_list = vectorizer.get_feature_names()\n",
    "        self.idf = vectorizer._tfidf.idf_\n",
    "        \n",
    "        self.word_to_index = {}\n",
    "        for index, w in enumerate(self.words_list):\n",
    "            self.word_to_index[w] = index\n",
    "    \n",
    "    def use_idf(self, t):\n",
    "        return (t.is_alpha and \n",
    "                not (t.is_space or t.is_punct or \n",
    "                     t.is_stop or t.like_num))\n",
    "    \n",
    "    def get_idf(self, token):\n",
    "        if not self.use_idf(token):\n",
    "            return 1\n",
    "        return self.get_word_idf(token.text)\n",
    "    \n",
    "    def get_word_idf(self, word):\n",
    "        if word in self.word_to_index:\n",
    "            idf = self.idf[self.word_to_index[word]]\n",
    "        else:\n",
    "            # https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/feature_extraction/text.py#L1443\n",
    "            idf = np.log(self.corpus_len + 1 / 1) + 1\n",
    "#         print(\"Calling idf for \" + word + \" = \" + str(idf))\n",
    "        return idf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idf_model = TfIdf(DataGenerator.get_test_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubtreeFeatureGeneratorIdf:\n",
    "  NAME = 'SubtreeFeatureIdf'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def get_feature_for_length(self, g_f1, g_f2, length):\n",
    "    f1 = g_f1.get_subtree_features(length=length, idf_model=idf_model)\n",
    "    f2 = g_f2.get_subtree_features(length=length, idf_model=idf_model)\n",
    "    \n",
    "    score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity=self.SIMILARITY)\n",
    "\n",
    "    norm = len(f1) + len(f2)\n",
    "    return (score * 2.) / norm if norm != 0 else 0\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.get_feature_for_length(g_f1, g_f2, 0),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 1),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 2),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 3),\n",
    "      self.get_feature_for_length(g_f1, g_f2, 4),\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "subtree_feature_generator_idf = SubtreeFeatureGeneratorIdf()\n",
    "# SGDClassifier {'precision': 71.5, 'recall': 95.38, 'f1': 81.73, 'accuracy': 71.65}\n",
    "# Not better than simple subtree_feature_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scores = base_classification_test(subtree_feature_generator_idf, force_feature_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeneralFeatures:\n",
    "    \n",
    "    @classmethod\n",
    "    def get_s_len(cls, s):\n",
    "        doc = nlp(s)\n",
    "        return np.array([len(doc)])\n",
    "    \n",
    "    @classmethod\n",
    "    def get_n_grams(cls, s, n, doc=None):\n",
    "        if doc is None:\n",
    "            d = nlp(s)\n",
    "        else:\n",
    "            d = doc\n",
    "        \n",
    "        res = []\n",
    "        count=0\n",
    "        for token in d[:len(d)-n+1]:  \n",
    "           res.append(d[count:count+n])  \n",
    "           count=count+1  \n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NodeSimilarity:\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def basic(cls, node1, node2):\n",
    "        if (node1['token'] is None or \n",
    "            node2['token'] is None or \n",
    "            not node1['token'].has_vector or \n",
    "            not node2['token'].has_vector\n",
    "           ):\n",
    "            return 1 if node1['node'] == node2['node'] else 0\n",
    "        else:\n",
    "            return Vector.similarity(\n",
    "                node1['token'].vector, \n",
    "                node2['token'].vector\n",
    "            )\n",
    "        \n",
    "    @classmethod\n",
    "    def token_similarity(cls, token1, token2):\n",
    "        \"\"\"\n",
    "        It's spacy tokens\n",
    "        \"\"\"\n",
    "        if token1.has_vector and token2.has_vector:\n",
    "            return Vector.similarity(\n",
    "                token1.vector, \n",
    "                token2.vector\n",
    "            )\n",
    "        else:\n",
    "            return 1 if token1.text == token2.text else 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token as SpacyToken\n",
    "\n",
    "class NGramSimilarity:\n",
    "    \n",
    "    @classmethod\n",
    "    def basic_word(cls, n_gram_1, n_gram_2):\n",
    "        \"\"\"\n",
    "            n_gram_1 is Token\n",
    "            n_gram_2 is Token\n",
    "        \"\"\"\n",
    "        if isinstance(n_gram_1[0], SpacyToken):\n",
    "            comparator = NodeSimilarity.token_similarity\n",
    "            \n",
    "        for i in range(len(n_gram_1)):\n",
    "            if comparator(n_gram_1[i], n_gram_2[i]) < 0.9:\n",
    "                return False\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BrevityPenalty:\n",
    "    \n",
    "    @classmethod\n",
    "    def compute(cls, ref_length, hyp_length):\n",
    "        \"\"\"\n",
    "            ref_lengths - int\n",
    "            hyp_lengths - int\n",
    "            Return BrevityPenalty - double\n",
    "            https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "        \"\"\"\n",
    "        \n",
    "        if hyp_length > ref_length:\n",
    "            return 1\n",
    "        # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n",
    "        elif hyp_length == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.exp(1 - ref_length / hyp_length)\n",
    "\n",
    "\n",
    "class BLEUCalculator:\n",
    "    \n",
    "    @classmethod\n",
    "    def precision(cls, reference, hypothesis, get_n_grams_funct, is_n_gram_equal_func, n):\n",
    "        \"\"\"\n",
    "        s1 - First sentance\n",
    "        s2 - Second sentance\n",
    "        get_n_grams_funct - Function that takes:\n",
    "            s - sentance\n",
    "            n - size of n gram\n",
    "            Return list of n_grams\n",
    "        is_n_gram_equal_func - n_gram comparator\n",
    "            n_gram_1\n",
    "            n_gram_2\n",
    "            Return Bool\n",
    "        n - size of bigram\n",
    "\n",
    "        \"\"\"\n",
    "        # Extracts all ngrams in hypothesis\n",
    "        # Set an empty Counter if hypothesis is empty.\n",
    "        \n",
    "        reference_n_grams = get_n_grams_funct(reference, n)\n",
    "\n",
    "        hypothesis_n_grams = get_n_grams_funct(hypothesis, n)\n",
    "\n",
    "        total_found = 0\n",
    "\n",
    "        for n_gram_h in hypothesis_n_grams:\n",
    "            found = False\n",
    "            for n_gram_r in reference_n_grams:\n",
    "#                 print(n_gram_h)\n",
    "#                 print(n_gram_r)\n",
    "                if is_n_gram_equal_func(n_gram_h, n_gram_r):\n",
    "#                     print(n_gram_h)\n",
    "#                     print(n_gram_r)\n",
    "#                     print(\"Match\")\n",
    "#                     print(\"*\" * 20)\n",
    "                    \n",
    "                    found = True\n",
    "            if found:\n",
    "                total_found += 1\n",
    "\n",
    "        numerator = total_found\n",
    "        # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n",
    "        denominator = max(1, len(hypothesis_n_grams))\n",
    "\n",
    "        return (numerator * 1.) / denominator\n",
    "    \n",
    "    @classmethod\n",
    "    def compute(cls, reference, hypothesis, get_n_grams_funct, is_n_gram_equal_func, max_n):\n",
    "        \"\"\"\n",
    "        s1 - First sentance\n",
    "        s2 - Second sentance\n",
    "        get_n_grams_funct - Function that takes:\n",
    "            s - sentance\n",
    "            n - size of n gram\n",
    "            Return list of n_grams\n",
    "        is_n_gram_equal_func - n_gram comparator\n",
    "            n_gram_1\n",
    "            n_gram_2\n",
    "            Return Bool\n",
    "        max_n - Max size of bigram\n",
    "        \n",
    "        Return BLEU - double\n",
    "        \n",
    "        https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "        \"\"\"\n",
    "        \n",
    "        p_n = []\n",
    "        \n",
    "        weight = 1. / max_n\n",
    "            \n",
    "        weights = [weight] * max_n\n",
    "        \n",
    "        # For each order of ngram, calculate the numerator and\n",
    "        # denominator for the corpus-level modified precision.\n",
    "        for i, _ in enumerate(weights, start=1):\n",
    "            _p = cls.precision(reference, hypothesis, get_n_grams_funct, is_n_gram_equal_func, i)\n",
    "            if abs(_p) < 0.001:\n",
    "                return 0\n",
    "            p_n.append(_p)\n",
    "\n",
    "        hyp_lengths = GeneralFeatures.get_s_len(hypothesis)\n",
    "        ref_lengths = GeneralFeatures.get_s_len(reference)\n",
    "\n",
    "        # Calculate brevity penalty.\n",
    "        bp = BrevityPenalty.compute(ref_lengths, hyp_lengths)\n",
    "        \n",
    "        s = [w_i * math.log(p_i) for w_i, p_i in zip(weights, p_n)]\n",
    "        s = bp * math.exp(math.fsum(s))\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    @classmethod\n",
    "    def test_precision(cls):\n",
    "        \"\"\"\n",
    "            Example from https://leimao.github.io/blog/BLEU-Score/\n",
    "        \n",
    "        \"\"\"\n",
    "        s1 = \"the cat is on the mat\"\n",
    "        s2 = \"the cat the cat on the mat\"\n",
    "        \n",
    "        p1 = cls.precision(\n",
    "            s1, \n",
    "            s2,\n",
    "            GeneralFeatures.get_n_grams,\n",
    "            NGramSimilarity.basic_word,\n",
    "            1\n",
    "        )\n",
    "        \n",
    "        assert( abs(p1 - 1.) < 0.001)\n",
    "        \n",
    "        p2 = cls.precision(\n",
    "            s1, \n",
    "            s2,\n",
    "            GeneralFeatures.get_n_grams,\n",
    "            NGramSimilarity.basic_word,\n",
    "            2\n",
    "        )\n",
    "        \n",
    "        assert( abs(p2 - 0.66666) < 0.001)\n",
    "        \n",
    "    @classmethod\n",
    "    def test_bleu(cls):\n",
    "        \"\"\"\n",
    "            Example from https://leimao.github.io/blog/BLEU-Score/\n",
    "        \n",
    "        \"\"\"\n",
    "        s1 = \"the cat is on the mat\"\n",
    "        s2 = \"the cat the cat on the mat\"\n",
    "        \n",
    "        bleu1 = cls.compute(\n",
    "            s1, \n",
    "            s2,\n",
    "            GeneralFeatures.get_n_grams,\n",
    "            NGramSimilarity.basic_word,\n",
    "            4\n",
    "        )\n",
    "        print(bleu1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BLEUCalculator.test_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MarchFeatureGenerator:\n",
    "    NAME = 'MarchFeature'\n",
    "\n",
    "    def get_feature_1(self, s1, s2):\n",
    "        len_s1 = GeneralFeatures.get_s_len(s1)\n",
    "        len_s2 = GeneralFeatures.get_s_len(s2)\n",
    "        \n",
    "        def f(len_s1, len_s2):\n",
    "            d_1 = (len_s1 - len_s2) * 1. / len_s1\n",
    "            d_2 = 1./ 0.8 ** (len_s1 - len_s2)\n",
    "            r = np.array([d_1, d_2])\n",
    "            return r\n",
    "        \n",
    "        feature_1 = np.array([])\n",
    "        feature_1 = np.append(feature_1, f(len_s1, len_s2))\n",
    "        feature_1 = np.append(feature_1, f(len_s2, len_s1))\n",
    "        return feature_1\n",
    "    \n",
    "    def get_feature_2(self, s1, s2):\n",
    "        doc_1 = nlp(s1)\n",
    "        doc_2 = nlp(s2)\n",
    "        \n",
    "        def compare_n_grams(s1, s2, doc_1, doc_2,  n):\n",
    "            s1_list = GeneralFeatures.get_n_grams(s1, n, doc_1)\n",
    "            s2_list = GeneralFeatures.get_n_grams(s2, n, doc_2)\n",
    "            \n",
    "            def is_n_gram_equal(n_gram_1, n_gram_2):\n",
    "                for i in range(len(n_gram_1)):\n",
    "                    if n_gram_1[i].text != n_gram_2[i].text:\n",
    "                        if n_gram_1[i].similarity(n_gram_2[i]) < 0.9:\n",
    "                            return False\n",
    "                return True\n",
    "            \n",
    "            count = 0\n",
    "            for n_gram_1 in s1_list:\n",
    "                match = False\n",
    "                for n_gram_2 in s2_list:\n",
    "                    if is_n_gram_equal(n_gram_1, n_gram_2):\n",
    "                        match = True\n",
    "                if match:\n",
    "                    count += 1\n",
    "            d = count * 1. / len(s1_list)\n",
    "            return np.array([d])\n",
    "        \n",
    "        feature_2 = np.array([])\n",
    "        \n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s1, s2, doc_1, doc_2, 1))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s2, s1, doc_2, doc_1, 1))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s1, s2, doc_1, doc_2, 2))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s2, s1, doc_2, doc_1, 2))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s1, s2, doc_1, doc_2, 3))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s2, s1, doc_2, doc_1, 3))\n",
    "        \n",
    "        return feature_2\n",
    "    \n",
    "    def get_feature_4(self, s1, s2):\n",
    "        g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "        g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "        g_f1 = GraphFeatures(g1)\n",
    "        g_f2 = GraphFeatures(g2)\n",
    "        \n",
    "        f1 = g_f1.get_simple_edge_features()\n",
    "        f2 = g_f2.get_simple_edge_features()\n",
    "        \n",
    "        def edge_similarity(edge1, edge2):\n",
    "            return (\n",
    "                (edge1['dependancy_type'] == edge2['dependancy_type']) \n",
    "                and NodeSimilarity.basic(edge1['start_node'], edge2['start_node']) > 0.9\n",
    "                and NodeSimilarity.basic(edge1['end_node'], edge2['end_node']) > 0.9\n",
    "            )\n",
    "        \n",
    "        def get_dependancy_similarity(f1, f2):\n",
    "            similarity_score = 0\n",
    "            \n",
    "#             def print_edge(edge):\n",
    "#                 print (edge['start_node']['node'] + \" <\" + edge['dependancy_type'] +  \"> \" + edge['end_node']['node'])\n",
    "            \n",
    "#             print(\"First tree\")\n",
    "#             for edge1 in f1:\n",
    "#                 print_edge(edge1)\n",
    "            \n",
    "#             print(\"*\" * 20)\n",
    "#             print(\"Second tree\")\n",
    "            \n",
    "#             for edge2 in f2:\n",
    "#                 print_edge(edge2)\n",
    "                        \n",
    "            for edge1 in f1:\n",
    "                match = False\n",
    "                for edge2 in f2:\n",
    "                    if edge_similarity(edge1, edge2):\n",
    "                        match = True\n",
    "                if match:\n",
    "                    similarity_score += 1\n",
    "\n",
    "            similarity_score = (similarity_score * 1.) / len(f1)\n",
    "            \n",
    "            return np.array([similarity_score])\n",
    "    \n",
    "        feature_4 = np.array([])\n",
    "        feature_4 = np.append(feature_4, get_dependancy_similarity(f1, f2))\n",
    "        feature_4 = np.append(feature_4, get_dependancy_similarity(f2, f1))\n",
    "        \n",
    "\n",
    "        return feature_4\n",
    "    \n",
    "    def get_feature_5(self, s1, s2):\n",
    "        g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "        g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "        \n",
    "        def compare_n_grams(g1, g2, length):\n",
    "            # Length in traversal starts with 0\n",
    "            length = length - 1\n",
    "            traversal_1 = GraphTraversal(graph=g1)\n",
    "            traversal_2 = GraphTraversal(graph=g2)\n",
    "            \n",
    "            s1_list = traversal_1.get_all_paths_with_len(length=length)\n",
    "            s2_list = traversal_2.get_all_paths_with_len(length=length)\n",
    "\n",
    "            def is_n_gram_equal(g1, g2, n_gram_1, n_gram_2):\n",
    "                for i in range(len(n_gram_1)):\n",
    "                    if NodeSimilarity.basic(g1.nodes[n_gram_1[i]], g2.nodes[n_gram_2[i]]) < 0.9:\n",
    "                        return False\n",
    "                return True\n",
    "            \n",
    "            count = 0\n",
    "            for n_gram_1 in s1_list:\n",
    "                match = False\n",
    "                for n_gram_2 in s2_list:\n",
    "                    if is_n_gram_equal(g1, g2, n_gram_1, n_gram_2):\n",
    "                        match = True\n",
    "                if match:\n",
    "                    count += 1\n",
    "            d = count * 1. / len(s1_list)\n",
    "            return np.array([d])\n",
    "        \n",
    "        feature_5 = np.array([])\n",
    "        \n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 1))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 1))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 2))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 2))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 3))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 3))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 4))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 4))\n",
    "        \n",
    "        return feature_5\n",
    "    \n",
    "    \n",
    "    def get_feature_6(self, s1, s2):\n",
    "        \n",
    "        def get_bleu(s1, s2, n_grams):\n",
    "            return np.array([BLEUCalculator.compute(\n",
    "                s1, \n",
    "                s2,\n",
    "                GeneralFeatures.get_n_grams,\n",
    "                NGramSimilarity.basic_word,\n",
    "                n_grams\n",
    "            )])\n",
    "        \n",
    "\n",
    "        feature_6 = np.array([])\n",
    "        \n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 1))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 1))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 2))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 2))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 3))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 3))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 4))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 4))\n",
    "        \n",
    "        return feature_6\n",
    "        \n",
    "        \n",
    "    def get_features(self, s1, s2):\n",
    "        features = np.array([])\n",
    "        \n",
    "#         features = np.append(features, self.get_feature_1(s1, s2))\n",
    "#         features = np.append(features, self.get_feature_2(s1, s2))\n",
    "        \n",
    "#         features = np.append(features, self.get_feature_4(s1, s2))\n",
    "#         features = np.append(features, self.get_feature_5(s1, s2))\n",
    "        features = np.append(features, self.get_feature_6(s1, s2))\n",
    "    \n",
    "    \n",
    "        return features\n",
    "    \n",
    "march_feature_generator = MarchFeatureGenerator()\n",
    "# get_feature_1 + get_feature_2 \n",
    "# RidgeClassifierCV\n",
    "# {'precision': 75.63, 'recall': 88.49, 'f1': 81.56, 'accuracy': 73.39}\n",
    "# RidgeClassifierCV\n",
    "# {'precision': 75.32, 'recall': 88.58, 'f1': 81.41, 'accuracy': 73.1}\n",
    "# 1 + 2 + 4 + 5\n",
    "# SGDClassifier {'precision': 72.73, 'recall': 94.42, 'f1': 82.17, 'accuracy': 72.75} [False, False, True, False, False, True, False, False, False, True, False, True, True, True, False, True, True, True, False, True]\n",
    "# SVC {'precision': 73.06, 'recall': 93.64, 'f1': 82.08, 'accuracy': 72.81} [True, False, False, False, True, False, True, False, False, True, True, False, True, True, False, True, False, False, False, False]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# verbose_data(march_feature_generator)\n",
    "scores = base_classification_test(march_feature_generator, force_feature_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scores = base_classification_test(march_feature_generator, force_feature_update=True)\n",
    "# feature_selection(march_feature_generator, verbose=False, bitmask_amount=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "These sent seems to be very similar, but a few words a different: \n",
    "* one can easily substitute rose with jumped\n",
    "* numbers are simmilar :   2.11 ~ 1.63\n",
    "* numbers are simmilar :   21.03 ~ 21.51 \n",
    "\n",
    "To work ion this we can do:\n",
    "* enhance node comparison if it is numbers\n",
    "* try to match subtrees without main word (rose = jumped)\n",
    "\n",
    "```\n",
    "First tree\n",
    "ROOT <ROOT> rose\n",
    "The <det> stock\n",
    "stock <nsubj> rose\n",
    "rose <npadvmod> 2.11\n",
    "rose <punct> ,\n",
    "rose <cc> or\n",
    "rose <conj> percent\n",
    "rose <punct> ,\n",
    "rose <advcl> close\n",
    "rose <punct> .\n",
    "$ <nmod> 2.11\n",
    "about <advmod> 11\n",
    "11 <nummod> percent\n",
    "to <aux> close\n",
    "close <npadvmod> Friday\n",
    "close <prep> at\n",
    "at <pobj> 21.51\n",
    "$ <nmod> 21.51\n",
    "21.51 <prep> on\n",
    "on <pobj> Exchange\n",
    "the <det> Exchange\n",
    "New <compound> York\n",
    "York <compound> Exchange\n",
    "Stock <compound> Exchange\n",
    "********************\n",
    "Second tree\n",
    "ROOT <ROOT> jumped\n",
    "PG&E <compound> Corp.\n",
    "Corp. <compound> shares\n",
    "shares <nsubj> jumped\n",
    "jumped <npadvmod> 1.63\n",
    "jumped <prep> to\n",
    "jumped <prep> on\n",
    "jumped <prep> on\n",
    "jumped <punct> .\n",
    "$ <nmod> 1.63\n",
    "1.63 <cc> or\n",
    "1.63 <conj> percent\n",
    "8 <nummod> percent\n",
    "to <pobj> 21.03\n",
    "$ <nmod> 21.03\n",
    "on <pobj> Exchange\n",
    "the <det> Exchange\n",
    "New <compound> York\n",
    "York <compound> Exchange\n",
    "Stock <compound> Exchange\n",
    "on <pobj> Friday\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.406453,
   "end_time": "2020-10-17T06:05:26.039936",
   "environment_variables": {},
   "exception": null,
   "input_path": "Server version Dependancy parser.ipynb",
   "output_path": "Out1.ipynb",
   "parameters": {},
   "start_time": "2020-10-17T06:05:23.633483",
   "version": "2.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}