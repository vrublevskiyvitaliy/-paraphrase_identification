{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best f1\n",
    "# LinearSVC {'precision': 74.57, 'recall': 94.59, 'f1': 83.4, 'accuracy': 74.96} [False, True, True, False, False, False, False, False, False, True, True, True, True, False, True, True, False, True, True, True, True, True, False, False, True, False, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, False, False, False, True, False, True, False, False, True, True, False, False, True, True, True, False, True, True, True, False, True, False, False, False, False, True, False, True, True, True, True, True, False, True, True, False, True, True, False, False, True, False, False, False, False, True, True, True, False, True, False, False, False, True, True, True, True, True, False, False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports and installs should be here\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import wordpunct_tokenize\n",
    "import operator\n",
    "import re, string\n",
    "import math\n",
    "import spacy\n",
    "import copy\n",
    "\n",
    "from nltk import Tree\n",
    "# Space module import\n",
    "import en_core_web_lg\n",
    "# NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
    "import networkx as nx\n",
    "from networkx import __version__ as nxv\n",
    "import networkx.algorithms as networkx_algorithms\n",
    "\n",
    "# linear_sum_assignment Hungarian algorithm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TfIdf:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.prepare_corpus()\n",
    "        self.fit()\n",
    "        \n",
    "    def prepare_corpus(self):\n",
    "        self.corpus = [x['s1'] for x in self.data] + [x['s2'] for x in self.data]\n",
    "        self.corpus = list(set(self.corpus))\n",
    "        self.corpus = sorted(self.corpus)\n",
    "        self.corpus_len = len(self.corpus)\n",
    "                \n",
    "        self.sent_to_index = {}\n",
    "        for index, s in enumerate(self.corpus):\n",
    "            self.sent_to_index[s] = index\n",
    "            \n",
    "    def fit(self):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        self.words_list = vectorizer.get_feature_names()\n",
    "        self.idf = vectorizer._tfidf.idf_\n",
    "        \n",
    "        self.word_to_index = {}\n",
    "        for index, w in enumerate(self.words_list):\n",
    "            self.word_to_index[w] = index\n",
    "    \n",
    "    def use_idf(self, t):\n",
    "        return (t.is_alpha and \n",
    "                not (t.is_space or t.is_punct or \n",
    "                     t.is_stop or t.like_num))\n",
    "    \n",
    "    def get_idf(self, token):\n",
    "        if not self.use_idf(token):\n",
    "            return 1\n",
    "        return self.get_word_idf(token.text)\n",
    "    \n",
    "    def get_word_idf(self, word):\n",
    "        if word in self.word_to_index:\n",
    "            idf = self.idf[self.word_to_index[word]]\n",
    "        else:\n",
    "            # https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/feature_extraction/text.py#L1443\n",
    "            idf = np.log(self.corpus_len + 1 / 1) + 1\n",
    "#         print(\"Calling idf for \" + word + \" = \" + str(idf))\n",
    "        return idf\n",
    "\n",
    "\n",
    "\n",
    "def get_data_location():\n",
    "  return \"./../dataset/msr-paraphrase-corpus/\"\n",
    "\n",
    "def add_start_end_sentence_tokens(s):\n",
    "  return \"%s %s %s\" % (SENTENCE_START_TOKEN, s, SENTENCE_END_TOKEN)\n",
    "\n",
    "def load_data(_preprocess_sentence=None, _train=False, _test=False):\n",
    "    \"Load the MSRP dataset.\"\n",
    "    loc = get_data_location()\n",
    "    trainloc = loc + 'msr_paraphrase_train.txt'\n",
    "    testloc = loc + 'msr_paraphrase_test.txt'\n",
    "\n",
    "    if _preprocess_sentence is None:\n",
    "      _preprocess_sentence = lambda x: x\n",
    "\n",
    "    sent1_train, sent2_train, sent1_test, sent2_test = [], [], [], []\n",
    "    label_train, label_dev, label_test = [], [], []\n",
    "\n",
    "    if _train:\n",
    "        with open(trainloc, 'r', encoding='utf8') as f:\n",
    "            f.readline()  # skipping the header of the file\n",
    "            for line in f:\n",
    "                text = line.strip().split('\\t')\n",
    "                sent1_train.append(_preprocess_sentence(text[3]))\n",
    "                sent2_train.append(_preprocess_sentence(text[4]))\n",
    "                label_train.append(int(text[0]))\n",
    "\n",
    "    if _test:\n",
    "        with open(testloc, 'r', encoding='utf8') as f:\n",
    "            f.readline()  # skipping the header of the file\n",
    "            for line in f:\n",
    "                text = line.strip().split('\\t')\n",
    "                sent1_test.append(_preprocess_sentence(text[3]))\n",
    "                sent2_test.append(_preprocess_sentence(text[4]))\n",
    "                label_test.append(int(text[0]))\n",
    "\n",
    "    if _train and _test:\n",
    "        return [sent1_train, sent2_train], [sent1_test, sent2_test], [label_train, label_test]\n",
    "    elif _train:\n",
    "        return [sent1_train, sent2_train], label_train\n",
    "    elif _test:\n",
    "        return [sent1_test, sent2_test], label_test\n",
    "    \n",
    "    \n",
    "\n",
    "class DataGenerator:\n",
    "  @classmethod\n",
    "  def get_train_data(cls):\n",
    "    [sent1_train, sent2_train], label_train = load_data(_preprocess_sentence=None, _train=True, _test=False)\n",
    "    return [\n",
    "      {\"s1\": item[0], \"s2\": item[1], \"label\": item[2]}\n",
    "      for item in zip(sent1_train, sent2_train, label_train)       \n",
    "    ]\n",
    "\n",
    "  @classmethod\n",
    "  def get_test_data(cls):\n",
    "    [sent1_test, sent2_test], label_test = load_data(_preprocess_sentence=None, _train=False, _test=True)\n",
    "    \n",
    "    return [\n",
    "      {\"s1\": item[0], \"s2\": item[1], \"label\": item[2]}\n",
    "      for item in zip(sent1_test, sent2_test, label_test)       \n",
    "    ]\n",
    "\n",
    "\n",
    "idf_model = TfIdf(DataGenerator.get_test_data())\n",
    "\n",
    "\n",
    "def get_spacy_module():\n",
    "  return en_core_web_lg.load()\n",
    "\n",
    "nlp = get_spacy_module()\n",
    "\n",
    "\n",
    "def get_dependancy_graph(s, display=False):\n",
    "  doc = nlp(s)\n",
    "  if display:\n",
    "    spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "  edges = []\n",
    "  nodes = [{\n",
    "      \"node\": \"ROOT\",\n",
    "      \"token\": None,\n",
    "      \"is_fake\": True, \n",
    "  }]\n",
    "  for token in doc:\n",
    "    nodes.append({\n",
    "        \"node\": token.text,\n",
    "        \"token\": token,\n",
    "        \"is_fake\": False,\n",
    "    })\n",
    "    if token.dep_ == \"ROOT\":\n",
    "      edges.append({\n",
    "        \"start\": \"ROOT\",\n",
    "        \"end\": token.text,\n",
    "        \"start_node_id\": 0,\n",
    "        \"end_node_id\": token.i + 1,\n",
    "        \"type\": token.dep_\n",
    "      })\n",
    "    else:\n",
    "      edges.append({\n",
    "        \"start\": token.head.text,\n",
    "        \"end\": token.text,\n",
    "        \"start_node_id\":  token.head.i + 1,\n",
    "        \"end_node_id\": token.i + 1,\n",
    "        \"type\": token.dep_\n",
    "      })\n",
    "  return {\"nodes\": nodes, \"edges\": edges}\n",
    "\n",
    "\n",
    "\n",
    "class HungarianGraphNodesMatcher:\n",
    "\n",
    "  def __init__(self, _g1, _g2, threshold=0.5):\n",
    "    self.g1 = _g1\n",
    "    self.g2 = _g2\n",
    "    self.node_threshold = threshold\n",
    "    self.create_cost_matrix()\n",
    "    self.solve_linear_sum_assignment()\n",
    "    self.match_nodes()\n",
    "\n",
    "  def set_threshold(self, threshold):\n",
    "    self.node_threshold = threshold\n",
    "    self.match_nodes()\n",
    "\n",
    "  def create_cost_matrix(self):\n",
    "    self.matrix = np.zeros((len(self.g1[\"nodes\"]), len(self.g2[\"nodes\"])))\n",
    "    for i1, n1 in enumerate(self.g1[\"nodes\"]):\n",
    "       for i2, n2 in enumerate(self.g2[\"nodes\"]):\n",
    "         if (not n1[\"is_fake\"] and not n2[\"is_fake\"] and \n",
    "            n1[\"token\"].has_vector and n2[\"token\"].has_vector):\n",
    "           self.matrix[i1][i2] = n1[\"token\"].similarity(n2[\"token\"])\n",
    "         elif n1[\"is_fake\"] == n2[\"is_fake\"]:\n",
    "           self.matrix[i1][i2] = n1[\"node\"] == n2[\"node\"]\n",
    "         else:\n",
    "           self.matrix[i1][i2] = 0\n",
    "\n",
    "    # Now we need to fleep scores, because Hungarian is trying to minimize\n",
    "    self.cost = np.subtract(np.full(self.matrix.shape, 1), self.matrix)\n",
    "\n",
    "  def get_pandas_matrix(self):\n",
    "    df = pd.DataFrame(\n",
    "        data=self.matrix,\n",
    "        index=np.array([n[\"node\"] for n in self.g1[\"nodes\"]]),\n",
    "        columns=np.array([n[\"node\"] for n in self.g2[\"nodes\"]])\n",
    "      )\n",
    "    \n",
    "    return df\n",
    "\n",
    "  def solve_linear_sum_assignment(self):\n",
    "    row_ind, col_ind = linear_sum_assignment(self.cost)\n",
    "    \n",
    "    self.row_ind = row_ind\n",
    "    self.col_ind = col_ind\n",
    "\n",
    "  def match_nodes(self):\n",
    "    self.graph1_to_graph2 = {\n",
    "        item[0]: item[1] \n",
    "        for item in zip(self.row_ind, self.col_ind)\n",
    "        if self.matrix[item[0]][item[1]] > self.node_threshold\n",
    "    }\n",
    "\n",
    "  def create_node_aliases(self):\n",
    "    for id1, n1 in enumerate(self.g1[\"nodes\"]):\n",
    "      n1[\"alias\"] = \"G1_\" + str(id1) + n1[\"node\"]\n",
    "    for id2, n2 in enumerate(self.g2[\"nodes\"]):\n",
    "      n2[\"alias\"] = \"G2_\" + str(id2) + n2[\"node\"] \n",
    "    for id1, id2 in self.graph1_to_graph2.items():\n",
    "      n1 = self.g1[\"nodes\"][id1]\n",
    "      n2 = self.g2[\"nodes\"][id2]\n",
    "      n1[\"alias\"] = \"G1_\" + str(id1) + \"_\" + n1[\"node\"] + \"_G2_\" + str(id2) + \"_\" + n2[\"node\"]\n",
    "      n2[\"alias\"] = n1[\"alias\"]\n",
    "\n",
    "  def build_graph(self, g):\n",
    "    nx_g = nx.Graph()\n",
    "    for edge in g[\"edges\"]:\n",
    "      start_node = g[\"nodes\"][edge[\"start_node_id\"]]\n",
    "      end_node = g[\"nodes\"][edge[\"end_node_id\"]]\n",
    "      nx_g.add_edge(start_node[\"alias\"], end_node[\"alias\"])\n",
    "    return nx_g\n",
    "\n",
    "  def get_converted_graphs(self):\n",
    "    self.create_node_aliases()\n",
    "    g1 = self.build_graph(self.g1)\n",
    "    g2 = self.build_graph(self.g2)\n",
    "    return g1, g2\n",
    "\n",
    "  def print_matched_nodes(self):\n",
    "    print (\"Graph 1  =>   Graph 2\")\n",
    "    for id1, id2 in self.graph1_to_graph2.items():\n",
    "      print(f\"{self.g1['nodes'][id1]['node']}    =>   {self.g2['nodes'][id2]['node']}\")\n",
    "\n",
    "class HungarianGraphFeatureGenerator:\n",
    "  NAME = 'HungarianGraph'\n",
    "\n",
    "  def get_features_for_graphs(self, node_matcher, similarity):\n",
    "    node_matcher.set_threshold(similarity)\n",
    "    g1, g2 = node_matcher.get_converted_graphs()\n",
    "    score_normalized = compare_graphs(g1, g2, False, True)\n",
    "    score_raw = compare_graphs(g1, g2, False, False)\n",
    "    return np.array([score_normalized, score_raw])\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = get_dependancy_graph(s1, False)\n",
    "    g2 = get_dependancy_graph(s2, False)\n",
    "    node_matcher = HungarianGraphNodesMatcher(g1, g2, 0.9)\n",
    "    \n",
    "    features = np.array([])\n",
    "    \n",
    "    for similarity in [0.8, 0.85,  0.90, 0.95]:\n",
    "      features = np.append(features, self.get_features_for_graphs(node_matcher, similarity))\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "class HungarianNodeFeatureGenerator:\n",
    "  NAME = 'HungarianNode'\n",
    "\n",
    "  def get_features_for_graphs(self, node_matcher, similarity):\n",
    "    node_matcher.set_threshold(similarity)\n",
    "    g1, g2 = node_matcher.get_converted_graphs()\n",
    "    n1, n2 = len(g1), len(g2)\n",
    "    num_matched_nodes = len(node_matcher.graph1_to_graph2)\n",
    "    percent_matched = num_matched_nodes * 2. / (n1 + n2)\n",
    "    features = np.array([n1, n2, percent_matched])\n",
    "    return features\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = get_dependancy_graph(s1, False)\n",
    "    g2 = get_dependancy_graph(s2, False)\n",
    "    node_matcher = HungarianGraphNodesMatcher(g1, g2, 0.9)\n",
    "    \n",
    "    features = np.array([])\n",
    "    \n",
    "    for similarity in [0.8, 0.85,  0.90, 0.95]:\n",
    "      features = np.append(features, self.get_features_for_graphs(node_matcher, similarity))\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "class PathFeatureGenerator:\n",
    "  NAME = 'PathSimilarity'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def get_feature_for_length(self, g_f1, g_f2, length):\n",
    "    f1 = g_f1.get_path_features(length=length)\n",
    "    f2 = g_f2.get_path_features(length=length)\n",
    "    \n",
    "    norm = len(f1) + len(f2)\n",
    "    \n",
    "    features = np.array([])\n",
    "    for similarity in [0.8, 0.85,  0.90, 0.95]:\n",
    "      score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity)\n",
    "      score = (score * 2.) / norm if norm != 0 else 0\n",
    "    \n",
    "      features = np.append(features, score)\n",
    "    \n",
    "    return features\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "    \n",
    "    features = np.array([])\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 0))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 1))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 2))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 3))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 4))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class SubtreeFeatureGenerator:\n",
    "  NAME = 'SubtreeFeature'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def get_feature_for_length(self, g_f1, g_f2, length):\n",
    "    f1 = g_f1.get_subtree_features(length=length)\n",
    "    f2 = g_f2.get_subtree_features(length=length)\n",
    "    \n",
    "    norm = len(f1) + len(f2)\n",
    "    \n",
    "    features = np.array([])\n",
    "    for similarity in [0.8, 0.85,  0.90, 0.95]:\n",
    "      score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity)\n",
    "      score = (score * 2.) / norm if norm != 0 else 0\n",
    "    \n",
    "      features = np.append(features, score)\n",
    "    \n",
    "    return features\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "    \n",
    "    features = np.array([])\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 0))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 1))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 2))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 3))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 4))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class RootNodeFeatureGenerator:\n",
    "  NAME = 'RootNodeFeature'\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "\n",
    "    root_node1 = GraphBuilder.get_root_node(g1)\n",
    "    root_node2 = GraphBuilder.get_root_node(g2)\n",
    "\n",
    "    if root_node1['token'].has_vector and root_node2['token'].has_vector:\n",
    "      score = root_node1['token'].similarity(root_node2['token'])\n",
    "    else:\n",
    "      score = 0\n",
    "\n",
    "    features = np.array([\n",
    "      score,\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class SimpleEdgeMatcher:\n",
    "  NAME = 'SimpleEdgeMatcher'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def simple_match_edges(self, g_f1, g_f2):\n",
    "    f1 = g_f1.get_simple_edge_features()\n",
    "    f2 = g_f2.get_simple_edge_features()\n",
    "    \n",
    "    score = 0\n",
    "    for edge1 in f1:\n",
    "        if (edge1['start_node']['token'] is None or \n",
    "            not edge1['start_node']['token'].has_vector\n",
    "            or edge1['end_node']['token'] is None\n",
    "            or not edge1['end_node']['token'].has_vector):\n",
    "                continue\n",
    "        for edge2 in f2:\n",
    "            if (edge2['start_node']['token'] is None \n",
    "            or not edge2['start_node']['token'].has_vector\n",
    "            or edge2['end_node']['token'] is None\n",
    "            or not edge2['end_node']['token'].has_vector):\n",
    "                continue\n",
    "            if (Vector.similarity(\n",
    "                    edge1['start_node']['token'].vector, \n",
    "                    edge2['start_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "                and Vector.similarity(\n",
    "                    edge1['end_node']['token'].vector, \n",
    "                    edge2['end_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "               ):\n",
    "                score += 1\n",
    "        \n",
    "    similarity_score = (1. * score) / (len(f1) * len(f2))\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.simple_match_edges(g_f1, g_f2)\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "class SimpleEdgeMatcherWithDependancy:\n",
    "  NAME = 'SimpleEdgeMatcherWithDependancy'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "\n",
    "  def simple_match_edges_with_dependancy_type(self, g_f1, g_f2):\n",
    "    f1 = g_f1.get_simple_edge_features()\n",
    "    f2 = g_f2.get_simple_edge_features()\n",
    "    \n",
    "    score = 0\n",
    "    total = 0\n",
    "    for edge1 in f1:\n",
    "        if (edge1['start_node']['token'] is None or \n",
    "            not edge1['start_node']['token'].has_vector\n",
    "            or edge1['end_node']['token'] is None\n",
    "            or not edge1['end_node']['token'].has_vector):\n",
    "                continue\n",
    "        for edge2 in f2:\n",
    "            if (edge2['start_node']['token'] is None \n",
    "            or not edge2['start_node']['token'].has_vector\n",
    "            or edge2['end_node']['token'] is None\n",
    "            or not edge2['end_node']['token'].has_vector):\n",
    "                continue\n",
    "            if (Vector.similarity(\n",
    "                    edge1['start_node']['token'].vector, \n",
    "                    edge2['start_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "                and Vector.similarity(\n",
    "                    edge1['end_node']['token'].vector, \n",
    "                    edge2['end_node']['token'].vector\n",
    "                ) > self.SIMILARITY\n",
    "               ):\n",
    "                if (edge1['dependancy_type'] == edge2['dependancy_type']):\n",
    "                    score += 1\n",
    "                total += 1\n",
    "        \n",
    "    similarity_score = 0 if total == 0 else (1. * score) / total\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      self.simple_match_edges_with_dependancy_type(g_f1, g_f2)\n",
    "    ])\n",
    "    \n",
    "    simple_edge_matcher_feature_generator = SimpleEdgeMatcher()\n",
    "    features = np.append(features, simple_edge_matcher_feature_generator.get_features(s1, s2))\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class SimpleApproximateBigramKernel:\n",
    "  \"\"\"\n",
    "  There was an error here while training!, probably better to remove this feature.\n",
    "        From https://www.aclweb.org/anthology/L16-1452.pdf\n",
    "        Simple Approximate Bigram Kernel (SABK)\n",
    "  \"\"\"\n",
    "\n",
    "  NAME = 'SimpleApproximateBigramKernel'\n",
    "  EDGE_SIMILARITY_SCORE = 2\n",
    "\n",
    "  @classmethod  \n",
    "  def node_similarity(cls, node1, node2):\n",
    "    if (node1['token'] is None or \n",
    "        node2['token'] is None or \n",
    "        not node1['token'].has_vector or \n",
    "        not node2['token'].has_vector\n",
    "       ):\n",
    "        return 1 if node1['node'] == node2['node'] else 0\n",
    "    else:\n",
    "        return Vector.similarity(\n",
    "            node1['token'].vector, \n",
    "            node2['token'].vector\n",
    "        )\n",
    "  @classmethod  \n",
    "  def edge_similarity(cls, edge1, edge2):\n",
    "    return SimpleApproximateBigramKernel.EDGE_SIMILARITY_SCORE if edge1['dependancy_type'] == edge2['dependancy_type'] else 1\n",
    "\n",
    "  @classmethod  \n",
    "  def similarity(cls, edge1, edge2):\n",
    "    start_node_similarity = cls.node_similarity(edge1['start_node'], edge2['start_node'])\n",
    "    end_node_similarity = cls.node_similarity(edge1['end_node'], edge2['end_node'])\n",
    "    \n",
    "    edge_similarity = cls.edge_similarity(edge1, edge2)\n",
    "    \n",
    "    return (start_node_similarity + end_node_similarity) * edge_similarity\n",
    "    \n",
    "  @classmethod  \n",
    "  def compute_simple_approximate_bigram_kernel(cls, g_f1, g_f2):\n",
    "    f1 = g_f1.get_simple_edge_features()\n",
    "    f2 = g_f2.get_simple_edge_features()\n",
    "    \n",
    "    similarity_score = 0\n",
    "\n",
    "    for edge1 in f1:\n",
    "        for edge2 in f2:\n",
    "            similarity_score += cls.similarity(edge1, edge2)\n",
    "        \n",
    "    similarity_score = (similarity_score * 1.) / (len(g_f1.g.nodes) + len(g_f2.g.nodes))\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "\n",
    "    features = np.array([\n",
    "      SimpleApproximateBigramKernel.compute_simple_approximate_bigram_kernel(g_f1, g_f2)\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class SubtreeFeatureGeneratorIdf:\n",
    "  NAME = 'SubtreeFeatureIdf'\n",
    "\n",
    "  SIMILARITY = 0.8\n",
    "    \n",
    "  def get_feature_for_length(self, g_f1, g_f2, length):\n",
    "    f1 = g_f1.get_subtree_features(length=length, idf_model=idf_model)\n",
    "    f2 = g_f2.get_subtree_features(length=length, idf_model=idf_model)\n",
    "    \n",
    "    norm = len(f1) + len(f2)\n",
    "    \n",
    "    features = np.array([])\n",
    "    for similarity in [0.8, 0.85,  0.90, 0.95]:\n",
    "      score = MatchFeatureVectors.match_feature_vectors(f1, f2, similarity)\n",
    "      score = (score * 2.) / norm if norm != 0 else 0\n",
    "    \n",
    "      features = np.append(features, score)\n",
    "    \n",
    "    return features\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "    g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "    g_f1 = GraphFeatures(g1)\n",
    "    g_f2 = GraphFeatures(g2)\n",
    "    \n",
    "    features = np.array([])\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 0))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 1))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 2))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 3))\n",
    "    features = np.append(features, self.get_feature_for_length(g_f1, g_f2, 4))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class MarchFeatureGenerator:\n",
    "    NAME = 'MarchFeature'\n",
    "\n",
    "    def get_feature_1(self, s1, s2):\n",
    "        len_s1 = GeneralFeatures.get_s_len(s1)\n",
    "        len_s2 = GeneralFeatures.get_s_len(s2)\n",
    "        \n",
    "        def f(len_s1, len_s2):\n",
    "            d_1 = (len_s1 - len_s2) * 1. / len_s1\n",
    "            d_2 = 1./ 0.8 ** (len_s1 - len_s2)\n",
    "            r = np.array([d_1, d_2])\n",
    "            return r\n",
    "        \n",
    "        feature_1 = np.array([])\n",
    "        feature_1 = np.append(feature_1, f(len_s1, len_s2))\n",
    "        feature_1 = np.append(feature_1, f(len_s2, len_s1))\n",
    "        return feature_1\n",
    "    \n",
    "    def get_feature_2(self, s1, s2):\n",
    "        doc_1 = nlp(s1)\n",
    "        doc_2 = nlp(s2)\n",
    "        \n",
    "        def compare_n_grams(s1, s2, doc_1, doc_2,  n):\n",
    "            s1_list = GeneralFeatures.get_n_grams(s1, n, doc_1)\n",
    "            s2_list = GeneralFeatures.get_n_grams(s2, n, doc_2)\n",
    "            \n",
    "            def is_n_gram_equal(n_gram_1, n_gram_2):\n",
    "                for i in range(len(n_gram_1)):\n",
    "                    if n_gram_1[i].text != n_gram_2[i].text:\n",
    "                        if n_gram_1[i].similarity(n_gram_2[i]) < 0.9:\n",
    "                            return False\n",
    "                return True\n",
    "            \n",
    "            count = 0\n",
    "            for n_gram_1 in s1_list:\n",
    "                match = False\n",
    "                for n_gram_2 in s2_list:\n",
    "                    if is_n_gram_equal(n_gram_1, n_gram_2):\n",
    "                        match = True\n",
    "                if match:\n",
    "                    count += 1\n",
    "            d = count * 1. / len(s1_list)\n",
    "            return np.array([d])\n",
    "        \n",
    "        feature_2 = np.array([])\n",
    "        \n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s1, s2, doc_1, doc_2, 1))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s2, s1, doc_2, doc_1, 1))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s1, s2, doc_1, doc_2, 2))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s2, s1, doc_2, doc_1, 2))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s1, s2, doc_1, doc_2, 3))\n",
    "        feature_2 = np.append(feature_2, compare_n_grams(s2, s1, doc_2, doc_1, 3))\n",
    "        \n",
    "        return feature_2\n",
    "    \n",
    "    def get_feature_4(self, s1, s2):\n",
    "        g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "        g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "    \n",
    "        g_f1 = GraphFeatures(g1)\n",
    "        g_f2 = GraphFeatures(g2)\n",
    "        \n",
    "        f1 = g_f1.get_simple_edge_features()\n",
    "        f2 = g_f2.get_simple_edge_features()\n",
    "        \n",
    "        def edge_similarity(edge1, edge2):\n",
    "            return (\n",
    "                (edge1['dependancy_type'] == edge2['dependancy_type']) \n",
    "                and NodeSimilarity.basic(edge1['start_node'], edge2['start_node']) > 0.9\n",
    "                and NodeSimilarity.basic(edge1['end_node'], edge2['end_node']) > 0.9\n",
    "            )\n",
    "        \n",
    "        def get_dependancy_similarity(f1, f2):\n",
    "            similarity_score = 0\n",
    "            \n",
    "            for edge1 in f1:\n",
    "                match = False\n",
    "                for edge2 in f2:\n",
    "                    if edge_similarity(edge1, edge2):\n",
    "                        match = True\n",
    "                if match:\n",
    "                    similarity_score += 1\n",
    "\n",
    "            similarity_score = (similarity_score * 1.) / len(f1)\n",
    "            \n",
    "            return np.array([similarity_score])\n",
    "    \n",
    "        feature_4 = np.array([])\n",
    "        feature_4 = np.append(feature_4, get_dependancy_similarity(f1, f2))\n",
    "        feature_4 = np.append(feature_4, get_dependancy_similarity(f2, f1))\n",
    "        \n",
    "\n",
    "        return feature_4\n",
    "    \n",
    "    def get_feature_5(self, s1, s2):\n",
    "        g1 = GraphBuilder.build_nx_graph_from_sentance(s1)\n",
    "        g2 = GraphBuilder.build_nx_graph_from_sentance(s2)\n",
    "        \n",
    "        def compare_n_grams(g1, g2, length):\n",
    "            # Length in traversal starts with 0\n",
    "            length = length - 1\n",
    "            traversal_1 = GraphTraversal(graph=g1)\n",
    "            traversal_2 = GraphTraversal(graph=g2)\n",
    "            \n",
    "            s1_list = traversal_1.get_all_paths_with_len(length=length)\n",
    "            s2_list = traversal_2.get_all_paths_with_len(length=length)\n",
    "\n",
    "            def is_n_gram_equal(g1, g2, n_gram_1, n_gram_2):\n",
    "                for i in range(len(n_gram_1)):\n",
    "                    if NodeSimilarity.basic(g1.nodes[n_gram_1[i]], g2.nodes[n_gram_2[i]]) < 0.9:\n",
    "                        return False\n",
    "                return True\n",
    "            \n",
    "            count = 0\n",
    "            for n_gram_1 in s1_list:\n",
    "                match = False\n",
    "                for n_gram_2 in s2_list:\n",
    "                    if is_n_gram_equal(g1, g2, n_gram_1, n_gram_2):\n",
    "                        match = True\n",
    "                if match:\n",
    "                    count += 1\n",
    "            d = count * 1. / len(s1_list)\n",
    "            return np.array([d])\n",
    "        \n",
    "        feature_5 = np.array([])\n",
    "        \n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 1))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 1))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 2))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 2))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 3))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 3))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g1, g2, 4))\n",
    "        feature_5 = np.append(feature_5, compare_n_grams(g2, g1, 4))\n",
    "        \n",
    "        return feature_5\n",
    "    \n",
    "    \n",
    "    def get_feature_6(self, s1, s2):\n",
    "        \n",
    "        def get_bleu(s1, s2, n_grams):\n",
    "            return np.array([BLEUCalculator.compute(\n",
    "                s1, \n",
    "                s2,\n",
    "                GeneralFeatures.get_n_grams,\n",
    "                NGramSimilarity.basic_word,\n",
    "                n_grams\n",
    "            )])\n",
    "        \n",
    "\n",
    "        feature_6 = np.array([])\n",
    "        \n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 1))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 1))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 2))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 2))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 3))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 3))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s1, s2, 4))\n",
    "        feature_6 = np.append(feature_6, get_bleu(s2, s1, 4))\n",
    "        \n",
    "        return feature_6\n",
    "        \n",
    "        \n",
    "    def get_features(self, s1, s2):\n",
    "        features = np.array([])\n",
    "        \n",
    "        features = np.append(features, self.get_feature_6(s1, s2))\n",
    "    \n",
    "    \n",
    "        return features\n",
    "    \n",
    "\n",
    "class MarchFeatureGeneratorWithoutBleu(MarchFeatureGenerator):\n",
    "    NAME = 'MarchFeatureGeneratorWithoutBleu'\n",
    "    \n",
    "    def get_features(self, s1, s2):\n",
    "        features = np.array([])\n",
    "        \n",
    "        features = np.append(features, self.get_feature_1(s1, s2))\n",
    "        features = np.append(features, self.get_feature_2(s1, s2))\n",
    "        features = np.append(features, self.get_feature_4(s1, s2))\n",
    "        features = np.append(features, self.get_feature_5(s1, s2))\n",
    "    \n",
    "        return features\n",
    "    \n",
    "class MarchFeatureGeneratorOnlyBleu(MarchFeatureGenerator):\n",
    "    NAME = 'MarchFeatureGeneratorOnlyBleu'\n",
    "    \n",
    "    def get_features(self, s1, s2):\n",
    "        features = np.array([])\n",
    "        \n",
    "        features = np.append(features, self.get_feature_6(s1, s2))\n",
    "    \n",
    "        return features\n",
    "\n",
    "\n",
    "class AllFeatureFinal:\n",
    "  NAME = 'AllFeatureFinal'\n",
    "\n",
    "  def get_features(self, s1, s2):\n",
    "    generators = [\n",
    "        HungarianGraphFeatureGenerator(),\n",
    "        HungarianNodeFeatureGenerator(),\n",
    "        PathFeatureGenerator(),\n",
    "        SubtreeFeatureGenerator(),\n",
    "        RootNodeFeatureGenerator(),\n",
    "        SimpleEdgeMatcher(),\n",
    "        SimpleEdgeMatcherWithDependancy(),\n",
    "        SimpleApproximateBigramKernel(),\n",
    "        SubtreeFeatureGeneratorIdf(),\n",
    "        MarchFeatureGeneratorWithoutBleu(),\n",
    "        MarchFeatureGeneratorOnlyBleu()\n",
    "    ]\n",
    "    features = np.array([])\n",
    "    for generator in generators:\n",
    "      features = np.append(features, generator.get_features(s1, s2))\n",
    "    return features\n",
    "\n",
    "\n",
    "# Code is taken from https://github.com/Jacobe2169/ged4py\n",
    "\n",
    "class EdgeGraph():\n",
    "    def __init__(self, init_node, nodes):\n",
    "        self.init_node=init_node\n",
    "        self.nodes_ = nodes\n",
    "\n",
    "    def nodes(self):\n",
    "        return self.nodes_\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.nodes)\n",
    "    def __len__(self):\n",
    "        return len(self.nodes_)\n",
    "\n",
    "class AbstractGraphEditDistance(object):\n",
    "    def __init__(self, g1, g2):\n",
    "        self.g1 = g1\n",
    "        self.g2 = g2\n",
    "\n",
    "    def normalized_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the graph edit distance between graph g1 & g2\n",
    "        The distance is normalized on the size of the two graphs.\n",
    "        This is done to avoid favorisation towards smaller graphs\n",
    "        \"\"\"\n",
    "        avg_graphlen = len(self.g1) + len(self.g2)\n",
    "        return self.distance() / avg_graphlen\n",
    "\n",
    "    def distance(self):\n",
    "        return sum(self.edit_costs())\n",
    "\n",
    "    def edit_costs(self):\n",
    "        cost_matrix = self.create_cost_matrix()\n",
    "        row_ind,col_ind = linear_sum_assignment(cost_matrix)\n",
    "        return [cost_matrix[row_ind[i]][col_ind[i]] for i in range(len(row_ind))]\n",
    "\n",
    "    def create_cost_matrix(self):\n",
    "        \"\"\"\n",
    "        Creates a |N+M| X |N+M| cost matrix between all nodes in\n",
    "        graphs g1 and g2\n",
    "        Each cost represents the cost of substituting,\n",
    "        deleting or inserting a node\n",
    "        The cost matrix consists of four regions:\n",
    "        substitute \t| insert costs\n",
    "        -------------------------------\n",
    "        delete \t\t| delete -> delete\n",
    "        The delete -> delete region is filled with zeros\n",
    "        \"\"\"\n",
    "        n = len(self.g1)\n",
    "        m = len(self.g2)\n",
    "        cost_matrix = np.zeros((n+m,n+m))\n",
    "        #cost_matrix = [[0 for i in range(n + m)] for j in range(n + m)]\n",
    "        nodes1 = self.g1.nodes() if float(nxv) < 2 else list(self.g1.nodes())\n",
    "        nodes2 = self.g2.nodes() if float(nxv) < 2 else list(self.g2.nodes())\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                cost_matrix[i,j] = self.substitute_cost(nodes1[i], nodes2[j])\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                cost_matrix[i+n,j] = self.insert_cost(i, j, nodes2)\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                cost_matrix[j,i+m] = self.delete_cost(i, j, nodes1)\n",
    "\n",
    "        self.cost_matrix = cost_matrix\n",
    "        return cost_matrix\n",
    "\n",
    "    def insert_cost(self, i, j):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def delete_cost(self, i, j):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def substitute_cost(self, nodes1, nodes2):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def print_matrix(self):\n",
    "        print(\"cost matrix:\")\n",
    "        for column in self.create_cost_matrix():\n",
    "            for row in column:\n",
    "                if row == sys.maxsize:\n",
    "                    print (\"inf\\t\")\n",
    "                else:\n",
    "                    print (\"%.2f\\t\" % float(row))\n",
    "            print(\"\")\n",
    "\n",
    "class EdgeEditDistance(AbstractGraphEditDistance):\n",
    "    \"\"\"\n",
    "    Calculates the graph edit distance between two edges.\n",
    "    A node in this context is interpreted as a graph,\n",
    "    and edges are interpreted as nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g1, g2):\n",
    "        AbstractGraphEditDistance.__init__(self, g1, g2)\n",
    "\n",
    "    def insert_cost(self, i, j, nodes2):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        return sys.maxsize\n",
    "\n",
    "    def delete_cost(self, i, j, nodes1):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        return sys.maxsize\n",
    "\n",
    "    def substitute_cost(self, edge1, edge2):\n",
    "        if edge1 == edge2:\n",
    "            return 0.\n",
    "        return 1\n",
    "\n",
    "class GraphEditDistance(AbstractGraphEditDistance):\n",
    "    def __init__(self, g1, g2):\n",
    "        AbstractGraphEditDistance.__init__(self, g1, g2)\n",
    "\n",
    "    def substitute_cost(self, node1, node2):\n",
    "        return self.relabel_cost(node1, node2) + self.edge_diff(node1, node2)\n",
    "\n",
    "    def relabel_cost(self, node1, node2):\n",
    "        if node1 == node2:\n",
    "            return 0.\n",
    "        else:\n",
    "            return 1.\n",
    "\n",
    "    def delete_cost(self, i, j, nodes1):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        return sys.maxsize\n",
    "\n",
    "    def insert_cost(self, i, j, nodes2):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        else:\n",
    "            return sys.maxsize\n",
    "\n",
    "    def pos_insdel_weight(self, node):\n",
    "        return 1\n",
    "\n",
    "    def edge_diff(self, node1, node2):\n",
    "        edges1 = list(self.g1.edge[node1].keys()) if float(nxv) < 2 else list(self.g1.edges(node1))\n",
    "        edges2 = list(self.g2.edge[node2].keys()) if float(nxv) < 2 else list(self.g2.edges(node2))\n",
    "        if len(edges1) == 0 or len(edges2) == 0:\n",
    "            return max(len(edges1), len(edges2))\n",
    "\n",
    "        edit_edit_dist = EdgeEditDistance(EdgeGraph(node1,edges1), EdgeGraph(node2,edges2))\n",
    "        return edit_edit_dist.normalized_distance()\n",
    "    \n",
    "def compare_graphs(g1, g2, print_details=False, use_normalized=True):\n",
    "    ged = GraphEditDistance(g1, g2)\n",
    "\n",
    "    if print_details:\n",
    "        ged.print_matrix()\n",
    "\n",
    "    return ged.normalized_distance() if use_normalized else ged.distance()\n",
    "\n",
    "\n",
    "class GraphBuilder:\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  @classmethod\n",
    "  def build_nx_graph_from_dt(cls, g):\n",
    "    nx_g = nx.Graph()\n",
    "    for index, node in enumerate(g[\"nodes\"]):\n",
    "      nx_g.add_node(index, node=node['node'], token=node['token'], is_fake=node['is_fake'])\n",
    "    for edge in g[\"edges\"]:\n",
    "      nx_g.add_edge(edge[\"start_node_id\"], edge[\"end_node_id\"], dependancy_type=edge[\"type\"])\n",
    "    return nx_g\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def build_nx_graph_from_sentance(cls, s):\n",
    "    graph = get_dependancy_graph(s, False)\n",
    "    return cls.build_nx_graph_from_dt(graph)\n",
    "\n",
    "  @classmethod\n",
    "  def get_root_node(cls, g):\n",
    "    main_root_node = [n for n, _ in g.adj[0].items()][0]\n",
    "    return g.nodes[main_root_node]\n",
    "\n",
    "\n",
    "\n",
    "class GraphFeatures:\n",
    "  def __init__(self, graph=None, sentance=None):\n",
    "    assert graph is not None or sentance is not None\n",
    "    if graph is not None:\n",
    "      self.g = graph\n",
    "    else:\n",
    "      self.g = GraphBuilder.build_nx_graph_from_sentance(sentance)\n",
    "  \n",
    "  def get_path_features(self, length=0):\n",
    "    traversal = GraphTraversal(graph=self.g)\n",
    "    pathes = traversal.get_all_paths_with_len(length=length)\n",
    "    \n",
    "    pathes_with_nodes = [\n",
    "      [self.g.nodes[node] for node in path ]\n",
    "      for path in pathes \n",
    "    ]\n",
    "\n",
    "    filtered_pathes_with_nodes = [\n",
    "      path \n",
    "      for path in pathes_with_nodes \n",
    "      if all(\n",
    "          node[\"token\"] is not None and node[\"token\"].has_vector \n",
    "          for node in path\n",
    "      )\n",
    "    ]\n",
    "\n",
    "    aggregated_vectors = [\n",
    "       sum([node[\"token\"].vector for node in path])\n",
    "       for path in filtered_pathes_with_nodes\n",
    "    ]\n",
    "\n",
    "    return aggregated_vectors\n",
    "\n",
    "  def get_subtree_features(self, length=0, remove_tree_without_vector=True, remove_stop_words=False, idf_model=None):\n",
    "    \"\"\"\n",
    "    Return list of vectors, where each vector represent one subtree.\n",
    "    Subtree is created by aggregating vectors in this subtree.\n",
    "\n",
    "    Keyword arguments:\n",
    "    length -- the real part (default 0.0)\n",
    "    remove_tree_without_vector -- remove whole tree if at least one vector inside it \n",
    "      is empty (non common word)\n",
    "    remove_stop_words - remove word from tree if it is stop word\n",
    "    idf_model - If present, multiply vector by word idf\n",
    "    \"\"\"\n",
    "    traversal = GraphTraversal(graph=self.g)\n",
    "    subtrees = traversal.get_all_subtrees_with_depth(length=length)\n",
    "    \n",
    "    subtrees_with_nodes = [\n",
    "      [self.g.nodes[node] for node in subtree ]\n",
    "      for subtree in subtrees\n",
    "    ]\n",
    "\n",
    "    if remove_tree_without_vector:\n",
    "      subtrees_with_nodes = [\n",
    "        subtree \n",
    "        for subtree in subtrees_with_nodes \n",
    "        if all(\n",
    "            node[\"token\"] is not None and node[\"token\"].has_vector \n",
    "            for node in subtree\n",
    "        )\n",
    "      ]\n",
    "\n",
    "    idf = lambda word: 1\n",
    "    \n",
    "    if idf_model is not None:\n",
    "        idf = lambda word: idf_model.get_idf(word)\n",
    "    \n",
    "    aggregated_vectors = [\n",
    "       sum([\n",
    "          node[\"token\"].vector * idf(node[\"token\"])\n",
    "          for node in subtree \n",
    "          # If remove_tree_without_vector == false\n",
    "          if node[\"token\"] is not None and node[\"token\"].has_vector\n",
    "          and (not remove_stop_words or not node[\"token\"].is_stop)\n",
    "        ])\n",
    "       for subtree in subtrees_with_nodes\n",
    "    ]\n",
    "\n",
    "    # Filter empty vectors\n",
    "    aggregated_vectors = [\n",
    "      v\n",
    "      for v in aggregated_vectors\n",
    "      if not np.isscalar(v)\n",
    "    ]\n",
    "\n",
    "    return aggregated_vectors\n",
    "\n",
    "  def get_simple_edge_features(self):\n",
    "    \"\"\"\n",
    "    Return list of edges\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for (start_idx, end_idx, dependancy_type) in self.g.edges.data('dependancy_type'):\n",
    "        item = {}\n",
    "        item['start_idx'] = start_idx\n",
    "        item['end_idx'] = end_idx\n",
    "        item['dependancy_type'] = dependancy_type\n",
    "        item['start_node'] = self.g.nodes[start_idx]\n",
    "        item['end_node'] = self.g.nodes[end_idx]\n",
    "        edges.append(item)\n",
    "        \n",
    "    return edges\n",
    "\n",
    "\n",
    "\n",
    "class GraphTraversal:\n",
    "  def __init__(self, graph=None, sentance=None):\n",
    "    assert graph is not None or sentance is not None\n",
    "    if graph is not None:\n",
    "      self.g = graph\n",
    "    else:\n",
    "      self.g = GraphBuilder.build_nx_graph_from_sentance(sentance)\n",
    "  \n",
    "  def get_paths_from_root_to_leafs(self, root=0):\n",
    "     #                 node. parent. path.      \n",
    "    res, stack = [], [(root, None, [])]\n",
    "    while stack:\n",
    "        node, parent, path = stack.pop()\n",
    "        path.append(node)\n",
    "        neighbours = [n for n, _ in self.g.adj[node].items()]\n",
    "        if len(neighbours) == 1 and neighbours[0] == parent:\n",
    "            res.append(path)\n",
    "        for n in neighbours:\n",
    "          if n == parent:\n",
    "            continue\n",
    "          stack.append((n, node, path[:]))\n",
    "    return res\n",
    "    \n",
    "  def get_all_paths_with_len(self, root=0, length=0):\n",
    "    \"\"\"\n",
    "    Return list of pathes with specificified len + 1.\n",
    "    The start is every node.\n",
    "    \n",
    "    For the tree:\n",
    "           1\n",
    "         2   3\n",
    "       5\n",
    "         6\n",
    "         \n",
    "    Len = 2:\n",
    "    [1, 2, 5]\n",
    "    [2, 5, 6]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #                  node. parent. path.                  \n",
    "    res, stack = [], [(root, None, [])]\n",
    "    started_new_path = {root}\n",
    "    while stack:\n",
    "        node, parent, path = stack.pop()\n",
    "        path.append(node)\n",
    "        neighbours = [n for n, _ in self.g.adj[node].items()]\n",
    "        if len(path) == length + 1:\n",
    "          res.append(path)\n",
    "        for n in neighbours:\n",
    "          if n == parent:\n",
    "            continue\n",
    "          if len(path) < length + 1:\n",
    "            stack.append((n, node, path[:]))\n",
    "          if n not in started_new_path:\n",
    "            stack.append((n, node, []))\n",
    "            started_new_path.add(n)\n",
    "        \n",
    "    return res\n",
    "\n",
    "  def get_all_subtrees_with_depth(self, root=0, parent=None, length=0):\n",
    "    \"\"\"\n",
    "      Return array of subtrees.\n",
    "      Each subtree is defined by indexes of their nodes.\n",
    "    \"\"\"\n",
    "    #                  node. parent. distance.  \n",
    "    res, stack = [], [(root, parent, 0)]\n",
    "    reached_depth = False\n",
    "    while stack:\n",
    "        node, _parent, distance = stack.pop()\n",
    "        res.append(node)\n",
    "        if distance >= length:\n",
    "          reached_depth = True\n",
    "          continue\n",
    "        neighbours = [n for n, _ in self.g.adj[node].items()]\n",
    "        for n in neighbours:\n",
    "          if n == _parent:\n",
    "            continue\n",
    "          stack.append((n, node, distance + 1))\n",
    "      \n",
    "    all_subtrees = []\n",
    "    if reached_depth:\n",
    "      all_subtrees.append(res)\n",
    "    for n, _ in self.g.adj[root].items():\n",
    "      if n == parent:\n",
    "        continue\n",
    "      all_subtrees += self.get_all_subtrees_with_depth(n, root, length)\n",
    "\n",
    "    return all_subtrees\n",
    "\n",
    "\n",
    "class Vector:\n",
    "  @classmethod\n",
    "  def get_norm(cls, v):\n",
    "    total = (v ** 2).sum()\n",
    "    return np.sqrt(total) if total != 0 else 0\n",
    "\n",
    "  @classmethod\n",
    "  def similarity(cls, v1, v2):\n",
    "    v1_norm = cls.get_norm(v1)\n",
    "    v2_norm = cls.get_norm(v2)\n",
    "    if v1_norm == 0 or v1_norm == 0:\n",
    "      return 0.0\n",
    "    return (np.dot(v1, v2) / (v1_norm * v2_norm))\n",
    "\n",
    "class MatchFeatureVectors:\n",
    "  @classmethod\n",
    "  def match_feature_vectors(cls, features1, features2, similarity=0.8):\n",
    "    \"\"\"\n",
    "      This function tries to do the following:\n",
    "      1) For each vector in features1 try to find whether vector with good similarity exist in features2.\n",
    "      Return ammount of matched vectors.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    features1_norm = []\n",
    "    for v1 in features1:\n",
    "      for v2 in features2:\n",
    "        score = Vector.similarity(v1, v2)\n",
    "        if score > similarity:\n",
    "          count += 1\n",
    "          break\n",
    "    return count\n",
    "\n",
    "class GeneralFeatures:\n",
    "    \n",
    "    @classmethod\n",
    "    def get_s_len(cls, s):\n",
    "        doc = nlp(s)\n",
    "        return np.array([len(doc)])\n",
    "    \n",
    "    @classmethod\n",
    "    def get_n_grams(cls, s, n, doc=None):\n",
    "        if doc is None:\n",
    "            d = nlp(s)\n",
    "        else:\n",
    "            d = doc\n",
    "        \n",
    "        res = []\n",
    "        count=0\n",
    "        for token in d[:len(d)-n+1]:  \n",
    "           res.append(d[count:count+n])  \n",
    "           count=count+1  \n",
    "        return res\n",
    "\n",
    "    \n",
    "class NodeSimilarity:\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def basic(cls, node1, node2):\n",
    "        if (node1['token'] is None or \n",
    "            node2['token'] is None or \n",
    "            not node1['token'].has_vector or \n",
    "            not node2['token'].has_vector\n",
    "           ):\n",
    "            return 1 if node1['node'] == node2['node'] else 0\n",
    "        else:\n",
    "            return Vector.similarity(\n",
    "                node1['token'].vector, \n",
    "                node2['token'].vector\n",
    "            )\n",
    "        \n",
    "    @classmethod\n",
    "    def token_similarity(cls, token1, token2):\n",
    "        \"\"\"\n",
    "        It's spacy tokens\n",
    "        \"\"\"\n",
    "        if token1.has_vector and token2.has_vector:\n",
    "            return Vector.similarity(\n",
    "                token1.vector, \n",
    "                token2.vector\n",
    "            )\n",
    "        else:\n",
    "            return 1 if token1.text == token2.text else 0\n",
    "        \n",
    "\n",
    "        \n",
    "class BrevityPenalty:\n",
    "    \n",
    "    @classmethod\n",
    "    def compute(cls, ref_length, hyp_length):\n",
    "        \"\"\"\n",
    "            ref_lengths - int\n",
    "            hyp_lengths - int\n",
    "            Return BrevityPenalty - double\n",
    "            https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "        \"\"\"\n",
    "        \n",
    "        if hyp_length > ref_length:\n",
    "            return 1\n",
    "        # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n",
    "        elif hyp_length == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.exp(1 - ref_length / hyp_length)\n",
    "\n",
    "\n",
    "class BLEUCalculator:\n",
    "    \n",
    "    @classmethod\n",
    "    def precision(cls, reference, hypothesis, get_n_grams_funct, is_n_gram_equal_func, n):\n",
    "        \"\"\"\n",
    "        s1 - First sentance\n",
    "        s2 - Second sentance\n",
    "        get_n_grams_funct - Function that takes:\n",
    "            s - sentance\n",
    "            n - size of n gram\n",
    "            Return list of n_grams\n",
    "        is_n_gram_equal_func - n_gram comparator\n",
    "            n_gram_1\n",
    "            n_gram_2\n",
    "            Return Bool\n",
    "        n - size of bigram\n",
    "\n",
    "        \"\"\"\n",
    "        # Extracts all ngrams in hypothesis\n",
    "        # Set an empty Counter if hypothesis is empty.\n",
    "        \n",
    "        reference_n_grams = get_n_grams_funct(reference, n)\n",
    "\n",
    "        hypothesis_n_grams = get_n_grams_funct(hypothesis, n)\n",
    "\n",
    "        total_found = 0\n",
    "\n",
    "        for n_gram_h in hypothesis_n_grams:\n",
    "            found = False\n",
    "            for n_gram_r in reference_n_grams:\n",
    "#                 print(n_gram_h)\n",
    "#                 print(n_gram_r)\n",
    "                if is_n_gram_equal_func(n_gram_h, n_gram_r):\n",
    "#                     print(n_gram_h)\n",
    "#                     print(n_gram_r)\n",
    "#                     print(\"Match\")\n",
    "#                     print(\"*\" * 20)\n",
    "                    \n",
    "                    found = True\n",
    "            if found:\n",
    "                total_found += 1\n",
    "\n",
    "        numerator = total_found\n",
    "        # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n",
    "        denominator = max(1, len(hypothesis_n_grams))\n",
    "\n",
    "        return (numerator * 1.) / denominator\n",
    "    \n",
    "    @classmethod\n",
    "    def compute(cls, reference, hypothesis, get_n_grams_funct, is_n_gram_equal_func, max_n):\n",
    "        \"\"\"\n",
    "        s1 - First sentance\n",
    "        s2 - Second sentance\n",
    "        get_n_grams_funct - Function that takes:\n",
    "            s - sentance\n",
    "            n - size of n gram\n",
    "            Return list of n_grams\n",
    "        is_n_gram_equal_func - n_gram comparator\n",
    "            n_gram_1\n",
    "            n_gram_2\n",
    "            Return Bool\n",
    "        max_n - Max size of bigram\n",
    "        \n",
    "        Return BLEU - double\n",
    "        \n",
    "        https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "        \"\"\"\n",
    "        \n",
    "        p_n = []\n",
    "        \n",
    "        weight = 1. / max_n\n",
    "            \n",
    "        weights = [weight] * max_n\n",
    "        \n",
    "        # For each order of ngram, calculate the numerator and\n",
    "        # denominator for the corpus-level modified precision.\n",
    "        for i, _ in enumerate(weights, start=1):\n",
    "            _p = cls.precision(reference, hypothesis, get_n_grams_funct, is_n_gram_equal_func, i)\n",
    "            if abs(_p) < 0.001:\n",
    "                return 0\n",
    "            p_n.append(_p)\n",
    "\n",
    "        hyp_lengths = GeneralFeatures.get_s_len(hypothesis)\n",
    "        ref_lengths = GeneralFeatures.get_s_len(reference)\n",
    "\n",
    "        # Calculate brevity penalty.\n",
    "        bp = BrevityPenalty.compute(ref_lengths, hyp_lengths)\n",
    "        \n",
    "        s = [w_i * math.log(p_i) for w_i, p_i in zip(weights, p_n)]\n",
    "        s = bp * math.exp(math.fsum(s))\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    @classmethod\n",
    "    def test_precision(cls):\n",
    "        \"\"\"\n",
    "            Example from https://leimao.github.io/blog/BLEU-Score/\n",
    "        \n",
    "        \"\"\"\n",
    "        s1 = \"the cat is on the mat\"\n",
    "        s2 = \"the cat the cat on the mat\"\n",
    "        \n",
    "        p1 = cls.precision(\n",
    "            s1, \n",
    "            s2,\n",
    "            GeneralFeatures.get_n_grams,\n",
    "            NGramSimilarity.basic_word,\n",
    "            1\n",
    "        )\n",
    "        \n",
    "        assert( abs(p1 - 1.) < 0.001)\n",
    "        \n",
    "        p2 = cls.precision(\n",
    "            s1, \n",
    "            s2,\n",
    "            GeneralFeatures.get_n_grams,\n",
    "            NGramSimilarity.basic_word,\n",
    "            2\n",
    "        )\n",
    "        \n",
    "        assert( abs(p2 - 0.66666) < 0.001)\n",
    "        \n",
    "    @classmethod\n",
    "    def test_bleu(cls):\n",
    "        \"\"\"\n",
    "            Example from https://leimao.github.io/blog/BLEU-Score/\n",
    "        \n",
    "        \"\"\"\n",
    "        s1 = \"the cat is on the mat\"\n",
    "        s2 = \"the cat the cat on the mat\"\n",
    "        \n",
    "        bleu1 = cls.compute(\n",
    "            s1, \n",
    "            s2,\n",
    "            GeneralFeatures.get_n_grams,\n",
    "            NGramSimilarity.basic_word,\n",
    "            4\n",
    "        )\n",
    "        print(bleu1)\n",
    "        \n",
    "\n",
    "from spacy.tokens import Token as SpacyToken\n",
    "\n",
    "class NGramSimilarity:\n",
    "    \n",
    "    @classmethod\n",
    "    def basic_word(cls, n_gram_1, n_gram_2):\n",
    "        \"\"\"\n",
    "            n_gram_1 is Token\n",
    "            n_gram_2 is Token\n",
    "        \"\"\"\n",
    "        if isinstance(n_gram_1[0], SpacyToken):\n",
    "            comparator = NodeSimilarity.token_similarity\n",
    "            \n",
    "        for i in range(len(n_gram_1)):\n",
    "            if comparator(n_gram_1[i], n_gram_2[i]) < 0.9:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "all_feature_generator_final = AllFeatureFinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vrublevskyi/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:656: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n"
     ]
    }
   ],
   "source": [
    "s1 = 'Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.'\n",
    "s2 = 'Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.'\n",
    "\n",
    "features = all_feature_generator_final.get_features(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC \n",
    "# {'precision': 74.57, 'recall': 94.59, 'f1': 83.4, 'accuracy': 74.96} \n",
    "bitmask = [False, True, True, False, False, False, False, False, False, True, True, True, True, False, True, True, False, True, True, True, True, True, False, False, True, False, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, False, False, False, True, False, True, False, False, True, True, False, False, True, True, True, False, True, True, True, False, True, False, False, False, False, True, False, True, True, True, True, True, False, True, True, False, True, True, False, False, True, False, False, False, False, True, True, True, False, True, False, False, False, True, True, True, True, True, False, False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bitmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vrublevskyi/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def prepare_data(feature_generator, force=False, limit=100000):\n",
    "  feature_name = feature_generator.NAME\n",
    "  global prepared_data\n",
    "  if feature_name not in prepared_data or force:\n",
    "    train_data = DataGenerator.get_train_data()\n",
    "    test_data = DataGenerator.get_test_data()\n",
    "    train_X = [\n",
    "      feature_generator.get_features(item['s1'], item['s2'])\n",
    "      for item in tqdm(train_data)\n",
    "    ]\n",
    "    train_Y = [item['label'] for item in train_data]\n",
    "\n",
    "    test_X = [\n",
    "      feature_generator.get_features(item['s1'], item['s2'])\n",
    "      for item in tqdm(test_data)\n",
    "    ]\n",
    "    test_Y = [item['label'] for item in test_data]\n",
    "    \n",
    "    features = {}\n",
    "    features['train_X'] = train_X\n",
    "    features['train_Y'] = train_Y\n",
    "    features['test_X'] = test_X\n",
    "    features['test_Y'] = test_Y\n",
    "    prepared_data[feature_name] = features\n",
    "\n",
    "def load_features():\n",
    "    file = 'features_final_n113.pickle'\n",
    "    global prepared_data\n",
    "    with open(file, 'rb') as f:\n",
    "        prepared_data = pickle.load(f)\n",
    "\n",
    "def get_metrics(feature_generator, classificator, force=False, limit=None, features_bitmap=None):\n",
    "  prepare_data(feature_generator, force, limit)\n",
    "  global prepared_data\n",
    "  feature_name = feature_generator.NAME\n",
    "\n",
    "  train_X = np.array(prepared_data[feature_name]['train_X'])\n",
    "  test_X = np.array(prepared_data[feature_name]['test_X'])\n",
    "  if features_bitmap is not None:\n",
    "    train_X = train_X[:, features_bitmap]\n",
    "    test_X = test_X[:, features_bitmap]\n",
    "\n",
    "  classificator.fit(\n",
    "    train_X,\n",
    "    prepared_data[feature_name]['train_Y']\n",
    "  )\n",
    "\n",
    "  test_Y_predicted = classificator.predict(test_X)\n",
    "\n",
    "  precision = precision_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "  recall = recall_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "  f1 = f1_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "  accuracy = accuracy_score(\n",
    "    prepared_data[feature_name]['test_Y'],\n",
    "    test_Y_predicted\n",
    "  )\n",
    "\n",
    "  return {\n",
    "    \"precision\" : round(precision * 100, 2),\n",
    "    \"recall\" : round(recall * 100, 2),\n",
    "    \"f1\" : round(f1 * 100, 2),\n",
    "    \"accuracy\" : round(accuracy * 100, 2),\n",
    "    \"classificator\": classificator,\n",
    "  }\n",
    "\n",
    "\n",
    "prepared_data = {}\n",
    "load_features()\n",
    "\n",
    "classificator = LinearSVC()\n",
    "score = get_metrics(all_feature_generator_final, classificator, features_bitmap=bitmask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 76.07, 'recall': 91.46, 'f1': 83.06, 'accuracy': 75.19, 'classificator': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)}\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model to disk\n",
    "# filename = 'finalized_model.sav'\n",
    "# pickle.dump(score[\"classificator\"], open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "\n",
    "\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(features[bitmask].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29056208, 0.70943792]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model._predict_proba_lr(features[bitmask].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "\n",
    "def predict(s1, s2):\n",
    "    feature_generator = AllFeatureFinal()\n",
    "    features = feature_generator.get_features(s1, s2)\n",
    "    \n",
    "    bitmask = [False, True, True, False, False, False, False, False, False, True, True, True, True, False, True, True, False, True, True, True, True, True, False, False, True, False, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, False, False, False, True, False, True, False, False, True, True, False, False, True, True, True, False, True, True, True, False, True, False, False, False, False, True, False, True, True, True, True, True, False, True, True, False, True, True, False, False, True, False, False, False, False, True, True, True, False, True, False, False, False, True, True, True, True, True, False, False, True, False]\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    features_bm = features[bitmask].reshape(1, -1)\n",
    "    is_paraphrase = loaded_model.predict(features_bm)[0] == 1\n",
    "    probabilities = loaded_model._predict_proba_lr(features_bm)\n",
    "    return {\n",
    "        'is_paraphrase': is_paraphrase,\n",
    "        'not_paraphrase_probability': probabilities[0][0],\n",
    "        'paraphrase_probability': probabilities[1],\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vrublevskyi/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:656: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-7f241b9bf16d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-4db62b006b9d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(s1, s2)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m'is_paraphrase'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mis_paraphrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m'not_paraphrase_probability'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;34m'paraphrase_probability'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     }\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "predict(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
